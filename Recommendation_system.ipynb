{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3sr4LSkzTuD"
      },
      "source": [
        "# item based collaborative filtering in KNN: MovieLens data 20M#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_my-JrJLzVRC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnrRKacftT17",
        "outputId": "92e04e7f-04f4-4dc8-ff7c-79500ffdf100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGKDG5tlvRKz"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"ml-latest.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-q_Puw7oMNe"
      },
      "source": [
        "Firstly, we need to organize \"ratings.csv\": it is a file containing \"user id\", \"movie id\", \"rating', \"timestamp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8fu619QiiNq"
      },
      "source": [
        "Data data frame (older version, it will be rating dataframe later):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awm48QUPw3wL",
        "outputId": "c4fc6d97-3bf9-43ad-adb1-a4d90a698a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      271200  0  2000  0.602    0.269  ...   95.687  4     26971.0  44.0  19\n",
            "0     218347  0  1978  0.275  0.28800  ...  116.900  3     32237.0  51.0  21\n",
            "1     163360  0  2018  0.701  0.86600  ...  128.001  4    136308.0  60.0  47\n",
            "2     211427  0  1983  0.132  0.00558  ...   78.637  4    987193.0  66.0  30\n",
            "3     278827  0  1978  0.559  0.60000  ...  128.402  4    313636.0  50.0  20\n",
            "4     166907  0  1965  0.519  0.48100  ...  109.246  4     78100.0  52.0  59\n",
            "...      ... ..   ...    ...      ...  ...      ... ..         ...   ...  ..\n",
            "9994  121893  0  1979  0.243  0.85500  ...  156.241  3  33483326.0  89.0  29\n",
            "9995  110107  0  1977  0.641  0.18500  ...  160.997  4     88541.0  68.0  43\n",
            "9996  190134  0  1971  0.272  0.44000  ...  107.487  4         0.0  18.0  17\n",
            "9997  107373  0  1966  0.630  0.04590  ...  115.351  3    135231.0  53.0  26\n",
            "9998  248933  0  1976  0.577  0.61000  ...  207.143  4   4919372.0  80.0  43\n",
            "\n",
            "[9999 rows x 18 columns]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of       271200  0  2000  0.602    0.269  ...   95.687  4     26971.0  44.0  19\n",
              "0     218347  0  1978  0.275  0.28800  ...  116.900  3     32237.0  51.0  21\n",
              "1     163360  0  2018  0.701  0.86600  ...  128.001  4    136308.0  60.0  47\n",
              "2     211427  0  1983  0.132  0.00558  ...   78.637  4    987193.0  66.0  30\n",
              "3     278827  0  1978  0.559  0.60000  ...  128.402  4    313636.0  50.0  20\n",
              "4     166907  0  1965  0.519  0.48100  ...  109.246  4     78100.0  52.0  59\n",
              "...      ... ..   ...    ...      ...  ...      ... ..         ...   ...  ..\n",
              "9994  121893  0  1979  0.243  0.85500  ...  156.241  3  33483326.0  89.0  29\n",
              "9995  110107  0  1977  0.641  0.18500  ...  160.997  4     88541.0  68.0  43\n",
              "9996  190134  0  1971  0.272  0.44000  ...  107.487  4         0.0  18.0  17\n",
              "9997  107373  0  1966  0.630  0.04590  ...  115.351  3    135231.0  53.0  26\n",
              "9998  248933  0  1976  0.577  0.61000  ...  207.143  4   4919372.0  80.0  43\n",
              "\n",
              "[9999 rows x 18 columns]>"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('./track_traning.csv', header=1)\n",
        "#data.columns = [\"user id\",\"item id\" ,\"rating\", \"timestamp\"]\n",
        "#data=data.drop(columns=['timestamp'])\n",
        "print(data)\n",
        "data.head#21627"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfy8K9eljGxM",
        "outputId": "fa2c49bd-f9ad-41d9-e556-423c0e5198d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD8mCKqCj_je"
      },
      "source": [
        "There are *283228 users* in total in our dataset, *53889* movies in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDrDq2eKf4Aq",
        "outputId": "4f98eb62-4eae-4d85-baae-e9703ada681d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "283228\n",
            "53889\n"
          ]
        }
      ],
      "source": [
        "print(data['user id'].nunique())\n",
        "print(data['item id'].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PubQ6yVo1B1F",
        "outputId": "332ad3fc-4aec-4743-933f-570acd25d1fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14161"
            ]
          },
          "execution_count": 79,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "user=random.sample(data['user id'].unique().tolist(),round(283228*0.05))\n",
        "len(user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ROYgdUueMFP"
      },
      "source": [
        "Filter user data frame to only include users contained in random sample of 0.1% of original dataset, called \"user\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "JhfsG1Cp10hW",
        "outputId": "676d462e-0d0b-45da-d24b-3987514385fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1385686, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user id</th>\n",
              "      <th>item id</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>17</td>\n",
              "      <td>32</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>17</td>\n",
              "      <td>780</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1587</th>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1589</th>\n",
              "      <td>19</td>\n",
              "      <td>9</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27744168</th>\n",
              "      <td>283117</td>\n",
              "      <td>120799</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27744169</th>\n",
              "      <td>283117</td>\n",
              "      <td>122882</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27744170</th>\n",
              "      <td>283117</td>\n",
              "      <td>122900</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27744171</th>\n",
              "      <td>283117</td>\n",
              "      <td>129354</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27744172</th>\n",
              "      <td>283117</td>\n",
              "      <td>132796</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1385686 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          user id  item id  rating\n",
              "1493           17       32     3.0\n",
              "1494           17      780     5.0\n",
              "1587           19        3     4.0\n",
              "1588           19        6     4.0\n",
              "1589           19        9     4.0\n",
              "...           ...      ...     ...\n",
              "27744168   283117   120799     5.0\n",
              "27744169   283117   122882     3.0\n",
              "27744170   283117   122900     5.0\n",
              "27744171   283117   129354     4.0\n",
              "27744172   283117   132796     1.5\n",
              "\n",
              "[1385686 rows x 3 columns]"
            ]
          },
          "execution_count": 80,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data=data[data['user id'].isin(user)]\n",
        "data=data[data['item id']!=1]\n",
        "print(data.shape)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRiCpG1ypT-d"
      },
      "source": [
        "Next, we capture tags information from tags.csv: each user commented on each movie using a word or a short phrase. The file contains \"user id\",\"movie id\", \"tag\", \"time stamp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pGP64v5ia0c"
      },
      "source": [
        "#Tags data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vICDYmQqp-hd",
        "outputId": "2e5450bd-5b3e-42dc-a956-043a940fe2e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['user id', 'item id', 'tag'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of          user id  item id            tag\n",
              "16            42    37733  disappointing\n",
              "17            42    37733      overrated\n",
              "18            42    37733         stupid\n",
              "386          332      236       Meg Ryan\n",
              "387          332     3408  Julia Roberts\n",
              "...          ...      ...            ...\n",
              "1108436   282989     2571   martial arts\n",
              "1108437   282989     2571          ulhas\n",
              "1108438   282989     6365        awesome\n",
              "1108672   283117     4011  Jason Statham\n",
              "1108673   283117     4011   twist ending\n",
              "\n",
              "[48289 rows x 3 columns]>"
            ]
          },
          "execution_count": 81,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags = pd.read_csv('./ml-latest/tags.csv', header=1)\n",
        "tags.columns = [\"user id\",\"item id\" ,\"tag\", \"timestamp\"]\n",
        "tags=tags.drop(columns=['timestamp'])\n",
        "tags=tags[tags['user id'].isin(user)]\n",
        "print(tags.columns)\n",
        "tags.head#21627"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKnkjC7MiX7C"
      },
      "source": [
        "#movie dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5yB6xiXxHtVQ",
        "outputId": "9b0e0b97-05f6-4ece-b3a2-17e9d9b82e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(58097, 3)\n",
            "Index(['movie id', 'title', 'genres'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie id</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movie id                               title                      genres\n",
              "0         2                      Jumanji (1995)  Adventure|Children|Fantasy\n",
              "1         3             Grumpier Old Men (1995)              Comedy|Romance\n",
              "2         4            Waiting to Exhale (1995)        Comedy|Drama|Romance\n",
              "3         5  Father of the Bride Part II (1995)                      Comedy"
            ]
          },
          "execution_count": 82,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies = pd.read_csv('./ml-latest/movies.csv', header=1)\n",
        "movies.columns = [\"movie id\",\"title\" ,\"genres\"]\n",
        "print(movies.shape)\n",
        "print(movies.columns)\n",
        "movies.head(4)\n",
        "#print(movies.head)\n",
        "#movies.columns\n",
        "# import re\n",
        "# item_name=\"movie id | movie title | release date | video release date |IMDb URL | unknown | Action | Adventure |\"+\\\n",
        "#             \"Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy |\"+\\\n",
        "#               \"Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\"+\\\n",
        "#               \"Thriller | War | Western\"\n",
        "# item_column=[x.strip() for x in item_name.split(\"|\")]\n",
        "# item=[]\n",
        "# item_file=open('./ml-100k/u.item',\"r\",encoding = \"ISO-8859-1\")\n",
        "# for line in item_file:\n",
        "#   new_line=re.split(\"\\|+\",line.rstrip())\n",
        "#   date=re.search(r'\\(([\\d]+)\\)',new_line[1])\n",
        "#   if date is None:\n",
        "#     date=\"\"\n",
        "#   else:\n",
        "#     date=date.group(1)\n",
        "#   new_line.insert(2,date)\n",
        "#   name=re.search(r'(.+)\\([\\d]+\\)',new_line[1])\n",
        "#   if name is not None:\n",
        "#     new_line[1]=name.group(1).strip()\n",
        "#   #print(len(new_line))\n",
        "#   item.append(new_line)\n",
        "# item=pd.DataFrame(item,columns=item_column)\n",
        "# item=item.drop(item.columns[[3,4]],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehnJn4q9lbz0"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "we can see there are no missing value in ratings, below is percentage of each rating by users: How each percentage distributed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "eC7d0syZZjFU",
        "outputId": "4410afc9-03c5-40fc-c949-42c516bf5705"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'popularity'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ae8ec1505a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'popularity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'popularity'"
          ]
        }
      ],
      "source": [
        "hist=data['popularity'].value_counts(normalize=True) * 100\n",
        "hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzBzj3-XoT1e",
        "outputId": "3c431a59-9c90-413a-cedb-f18692687121"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'popularity'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-33fa87c5aa86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'popularity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'popularity'"
          ]
        }
      ],
      "source": [
        "list(data.columns.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz2-UP2deply"
      },
      "source": [
        "How many ratings are given to each movie?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7wfS1yV2Huo",
        "outputId": "564b7683-435b-42ba-b40d-a8e5c128d53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22383\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Int64Index([     2,      3,      4,      5,      6,      7,      8,      9,\n",
              "                10,     11,\n",
              "            ...\n",
              "            193201, 193203, 193209, 193443, 193465, 193473, 193523, 193649,\n",
              "            193699, 193727],\n",
              "           dtype='int64', name='item id', length=22383)"
            ]
          },
          "execution_count": 84,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#We would like to obtain number of ratings per movie:\n",
        "num_of_rating=data.groupby(\"item id\")['rating'].count().tolist()\n",
        "print(len(num_of_rating))\n",
        "#data.groupby(\"item id\")['rating'].count()\n",
        "movie_id_sort=data.groupby(\"item id\")['rating'].count().index\n",
        "movie_id_sort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx7H-v1ZuGAd"
      },
      "source": [
        "1) Use simple KNN to find clusters of similiar users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "05W4XtgCMI_C",
        "outputId": "6017b592-b24c-446d-8d0d-1552915c8aba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'num of ratings')"
            ]
          },
          "execution_count": 85,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEGCAYAAABYV4NmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xcdb3/8deHRMAr0kNupNyABPlxrRgB67VSVSwg2IiIl59XvFcv+sMgcFEUjXhVjAoYAQ2I9JIgJaTRTA/pIZ30Xja97e7n98d8Z/fs7JQzs3Om7fv5eMxjz3znzDmfPbN7PvMt53vM3RERESm3A6odgIiINCYlGBERSYQSjIiIJEIJRkREEqEEIyIiiehZ7QCScPTRR3vfvn2rHYaISF2ZOnXqRnfvVa7tNWSC6du3L1OmTKl2GCIidcXMlpVze2oiExGRRCjBiIhIIpRgREQkEYkmGDNbamazzGy6mU0JZUea2UgzWxh+HhHKzcwGm9kiM5tpZqdHtjMgrL/QzAYkGbOIiJRHJWowH3H3d7p7//B8IDDa3fsBo8NzgPOAfuFxJXA7pBIScCNwJnAGcGM6KYmISO2qRhPZhcDQsDwU+Eyk/B5PmQAcbmZ9gHOAke6+2d23ACOBcysdtIiIFCfpBOPAc2Y21cyuDGW93X1NWF4L9A7LxwIrIu9dGcpylXdgZlea2RQzm7Jhw4Zy/g4iIlKCpK+D+YC7rzKzY4CRZjYv+qK7u5mV5X4B7j4EGALQv3//kre5aP12Nu3Yx5knHVWOsEREuq1EazDuvir8XA88TqoPZV1o+iL8XB9WXwUcH3n7caEsV3kiPv7rF7lkyISkNi8i0m0klmDM7A1m9sb0MnA2MBsYDqRHgg0AhoXl4cBlYTTZWcDW0JQ2AjjbzI4InftnhzIREalhSTaR9QYeN7P0fv7m7s+a2WTgITO7AlgGfCGs/zRwPrAI2AVcDuDum83sJ8DksN5N7r45wbhFRKQMEksw7r4EeEeW8k3Ax7KUO3BVjm3dDdxd7hhFRCQ5upJfREQSoQQjIiKJUIIREZFEKMGIiEgilGBERCQRSjAiIpIIJRgREUmEEoyIiCRCCUZERBKhBCMiIolQghERkUQowYiISCKUYEREJBFKMCIikgglmBx27WuudggiInVNCSaHv01cXu0QRETqmhKMiIgkQglGREQSoQQjIiKJUIIREZFEKMGIiEgilGBERCQRSjAiIpIIJRgREUmEEoyIiCRCCaaAlxdupLmltdphiIjUHSWYPMYt3shX7prI4NELqx2KiEjdUYLJY8P2vQC8tmlXlSMREak/SjAiIpIIJRgREUmEEoyIiCQi8QRjZj3MbJqZ/T08P9HMJprZIjN70MwODOUHheeLwut9I9u4NpTPN7Nzko5ZRES6rhI1mO8Ar0ae/wL4jbufDGwBrgjlVwBbQvlvwnqY2WnApcC/AucCt5lZj6SDdk96DyIijS3RBGNmxwEXAHeG5wZ8FHgkrDIU+ExYvjA8J7z+sbD+hcAD7r7X3V8DFgFnJBm3iIh0XdI1mFuBa4D0lYpHAU3unr7h/Urg2LB8LLACILy+NazfVp7lPW3M7Eozm2JmUzZs2FDu30NERIqUWIIxs08C6919alL7iHL3Ie7e39379+rVqxK7FBGRPHomuO33A582s/OBg4FDgd8Ch5tZz1BLOQ5YFdZfBRwPrDSznsBhwKZIeVr0PRVhldyZiEiDSKwG4+7Xuvtx7t6XVCf9GHf/MjAWuCisNgAYFpaHh+eE18e4u4fyS8MosxOBfsCkpOJuix/18ouIdEU1roP5AXC1mS0i1cdyVyi/CzgqlF8NDARw9znAQ8Bc4FngKndvSTrIzTv3J70LEZGGlmQTWRt3fx54PiwvIcsoMHffA1yc4/03AzcnF6GIiJSbruQXEZFEKMHkoD4YEZGuUYIREZFEKMGIiEgilGBERCQRSjAiIpIIJRgREUmEEoyIiCRCCSaHJl3JLyLSJUowOSzZuKPaIYiI1DUlmBx0R0sRka5RghERkUQowYiISCKUYCJaWju2i6mZTESkdEowEVt3Zx85ZrqlpYhI0ZRgREQkEUowIiKSCCUYERFJhBJMDup3ERHpGiWYHCYv3cKqpt3VDkNEpG4pweQxbvFGAOat2V7lSERE6k/BBGNmF5vZG8Py9Wb2mJmdnnxotWP+OiUYEZFixanB3ODu283sA8DHgbuA25MNqzboQksRkdLFSTAt4ecFwBB3fwo4MLmQRESkEcRJMKvM7I/AJcDTZnZQzPfVncyBYxpJJiJSujiJ4gvACOAcd28CjgT+X6JRiYhI3esZY52DgecBzOxIYC8wNsGYRESkAcSpwbwCbAAWAAvD8lIze8XM3p1kcCIiUr/iJJiRwPnufrS7HwWcBzwFfAu4LcngKq1Qn4u7s6+5tTLBiIjUuTgJ5ix3H5F+4u7PhbIJwEGJRVYFi9bvyPv6/ZNWcMr1z+gKfxGRGOIkmDVm9gMz+5fwuAZYZ2Y9gIb6Or9m6568rz85YzUAyzburEQ4IiJ1LU6C+RJwHPBEeJwQynqQGmGWlZkdbGaTzGyGmc0xsx+H8hPNbKKZLTKzB83swFB+UHi+KLzeN7Kta0P5fDM7p9RfVkREKqfgKDJ33wj8Z46XF+V5617go+6+w8xeB7xsZs8AVwO/cfcHzOwO4ApSMwNcAWxx95PN7FLgF8AlZnYacCnwr8CbgFFmdoq7t2TbaTnpSn4RkdLFmYvsFDMbYmbPmdmY9KPQ+zwl3anxuvBw4KPAI6F8KPCZsHxheE54/WNmZqH8AXff6+6vkUpqZ8T8/bpECUZEpHRxroN5GLgDuJP2aWNiCf00U4GTgT8Ai4Emd28Oq6wEjg3LxwIrANy92cy2AkeF8gmRzUbfE93XlcCVACeccEIxYUa2UdLbREQkizgJptndS5rcMjRjvdPMDgceB04tZTsx9zUEGALQv39/1T1ERKosTif/k2b2LTPrY2ZHph/F7CRMMTMWeC9wuJmlE9txwKqwvAo4HiC8fhiwKVqe5T0iIlKj4iSYAaTmHhtHqrlrKjCl0JvMrFeouWBmrwc+AbxKKtFcFNn2sLA8PDwnvD7G3T2UXxpGmZ0I9AMmxYhbRESqKM4oshNL3HYfYGjohzkAeMjd/25mc4EHzOynwDRS95ch/LzXzBYBm0mNHMPd55jZQ8BcoBm4qhIjyLJxOra8/W70Qj50Si/ecfzh1QhHRKSm5UwwZvZRdx9jZp/L9rq7P5Zvw+4+E3hXlvIlZBkF5u57gItzbOtm4OZ8+0tCrk7/pt372bWvmV+NXMCvRi5g6aALKhuYiEgdyFeD+TdgDPCpLK85kDfBNLJv3fcKxx7++mqHISJS03ImGHe/MSzeFK4/aRP6QhqOZdxyLN91MJqPTEQkvzid/I9mKXskS1ndy2wS27JrX3UCERFpAPn6YE4lNT3LYRn9MIeSuglZw5u3dnu1QxARqVv5+mDeAnwSOJyO/TDbgX9PMqhq0YX8IiLlk68PZhgwzMze6+7jKxiTiIg0gDhTxUwzs6tINZe1NY25+9cTi0pEROpenE7+e4F/Bs4BXiA1VUu37JzQ7MoiIvHFSTAnu/sNwE53HwpcAJyZbFgiIlLv4iSY/eFnk5m9ldQklMckF5KIiDSCOH0wQ8zsCOB6UhNPHgLckGhUVaIWMBGR8smbYMzsAGCbu28BXgROqkhUIiJS9/I2kbl7K3BNhWIREZEGEqcPZpSZfd/Mji/1hmP1QhdaioiUT5w+mEvCz6siZU4DNpflmp5fRESKl+QNxxqOBgGIiMQXp4msG1EVRkSkXJRgYvrEr1+odggiInUlZ4Ixs/eHnwdVLpzatXD9jmqHICJSV/LVYAaHn5pJWUREipavk3+/mQ0BjjWzwZkvuvt/JRdWbZr02uZqhyAiUjfyJZhPAh8nNYvy1MqEIyIijSLfDcc2Ag+Y2avuPqOCMYmISAOIM4psk5k9bmbrw+NRMzsu8cik4a3btod33vQcC9d1y9sLiTS8OAnmz6RmUX5TeDwZyhqOruSvrBFz1tK0az/3jF9W7VBEJAFxEswx7v5nd28Oj78AvRKOqyqUX0REyidOgtloZl8xsx7h8RVgU9KBiYhIfYuTYL4OfAFYC6wBLgIuTzIoERGpf3Emu1wGfLoCsYiISAPRXGTSrT08ZQVTl+kCWpEkxLkfjEjD+n+PzARg6aALqhyJSONJrAYT7oA51szmmtkcM/tOKD/SzEaa2cLw84hQbmY22MwWmdlMMzs9sq0BYf2FZjYgqZhFRKR8CtZgzOxw4DKgb3T9GHORNQPfc/dXzOyNwFQzGwl8DRjt7oPMbCAwEPgBcB7QLzzOBG4Hzgy3Z74R6E/qnl9TzWy4u28p5hcVEZHKitNE9jQwAZgFtMbdsLuvITXqDHffbmavAscCFwIfDqsNBZ4nlWAuBO5xdwcmmNnhZtYnrDvS3TcDhCR1LnB/3FhERKTy4iSYg9396q7sxMz6Au8CJgK9Q/KB1NDn3mH5WGBF5G0rQ1mu8sx9XAlcCXDCCSeUGmdJ7xMRkc7i9MHca2b/bmZ9Qv/JkaHZKhYzOwR4FPiuu2+LvhZqK2W51b27D3H3/u7ev1evhpxooGF5ef4ERKTGxEkw+4Bfkrrx2NTwmBJn42b2OlLJ5T53fywUrwtNX4Sf60P5KuD4yNuPC2W5yqXOqb4o0tjiJJjvASe7e193PzE8Tir0Jku1N90FvOruv468NBxIjwQbAAyLlF8WRpOdBWwNTWkjgLPN7Igw4uzsUFZ2OuGJiJRPnD6YRcCuErb9fuCrwCwzmx7KfggMAh4ysyuAZaSmoYHUYILzI/u7HMDdN5vZT4DJYb2b0h3+IiJSu+IkmJ3AdDMbC+xNFxYapuzuL5O7UvCxLOs7cFWObd0N3B0jVhERqRFxEswT4SEiIhJbnMkuh1YiEBERaSxxruR/jSxDieN09IuISPcVp4msf2T5YOBiIPZ1MPVE11mKiJRPwWHK7r4p8ljl7rcCDTn1rBKMiEj5xGkiOz3y9ABSNRpN8y8iInnFSRS/iiw3A0tpv3ZFpMtcM8WINKQ4o8g+UolApBtSm6RIQ4vTRHYQ8Hk63w/mpuTCqg7TZDEiImUTp4lsGLCV1CSXewusKyIiAsRLMMe5+7mJRyKJ2LO/hUuHTOAnF76Vtx13WLXDEZFuJM5syuPM7G2JR1IDunJfktVNu2ltrb3e6pkrtzJ9RRM3/X1OtUMRkW4mToL5ADDVzOab2Uwzm2VmM5MOrJ4s2bCD9w0aw+0vLK52KCIiNSNOE9l5iUdRI0rt5F/dtAeAcYs3ctVHTi5nSCIidSvOMOVllQhEkqVrTUSk0uI0kUkd06UmIlItSjBlpFqCiEg7JZgyqIdaQi3nvlqOTURKpwQT8drGndUOoeySzn0trY6XWHVLx7Zyy27ueGExz85eU77ARKTqlGAitu9prnYIdefNP3ya65+Y3aVtvLhgA4Oemcc3//pKmaISkVqgBBPR1aauWu6DKbWWAfDK8i3MXrU15+v3TVxe8rZFpHHpvi5lUMtdMOXoH/rcbeMAWDqoIe8zJyIJUQ0mopYThWTXd+BTfGPolE7lc1dv486XllQhIhFJUw0moh5Gg5WqhlvvumzUq+s6lZ0/+CUAvvHBkyodjogEqsGUUVcmy0xOA2dNEalpSjBl8OMn51Y7BBGRmqMEUwbz122vdghZ7dnf0jZ6rJZHuEnpavEWESJpSjAR1kCdMC2tzqk3PNvQtavufnKdt3YbJ/3waUbN7dwHJVILlGDKqJZqCc2trQDMynP9Sj1bs3U3J/3w6WqHUVXTlzcBMFIJRmqUEkw3UUO5ryyWbGi8aX1EGo0SjEidq83RiyIJJhgzu9vM1pvZ7EjZkWY20swWhp9HhHIzs8Fmtijclvn0yHsGhPUXmtmApOKVrtm0Y2/O174xdAoPTCrvdDKN01tWugbqMpQGlWQN5i/AuRllA4HR7t4PGB2eQ+q2zP3C40rgdkglJOBG4EzgDODGdFJKwgFd/I+t6e+RCXcQvfuno3K+NurVdQx8bFai+xeR2pNYgnH3F4HNGcUXAkPD8lDgM5HyezxlAnC4mfUBzgFGuvtmd98CjKRz0qoZSzbsqHYIXXbv+KVs2J67NhI1dVnmxwvNLa188JYxyU+9X4Pf3vc2t+SdFDTT/pZWNuap+cVVS4NLRKIq3QfT293TZ561QO+wfCywIrLeylCWq7wTM7vSzKaY2ZQNGzaUN+qYNu7YV5X9lsviDTu4Ydgcvv233NPmt7Q6H/7lWJ6etYYVm3d3en3Lrv2s2Ly7y1P4uztDxy3N2/RWa24cNodP/u5lVjV1Pi7ZfO+hGfT/6aiSh1tbLWZZkYiqdfJ76grAsn33cvch7t7f3fv36tWrXJvtVvY1p4Y2N+3an3OdHXuaWbppFwMfndnptaZd+7jmkRllieXVNdu5cfgcvvvg9Kyv1+LJdfqK1LDhbbtzH7+ov89cDdR406pIF1Q6wawLTV+En+tD+Srg+Mh6x4WyXOWJaKRO08wTcJyTWLqpJc5xyLa9W0ctZOz8/LXHFxdsiNXhv7+lcLKLY/22PXz0f59nxeZdXdpOLq2tzr3jl7K3uaWo41dOSlBSqyqdYIYD6ZFgA4BhkfLLwmiys4CtoSltBHC2mR0ROvfPDmVSLZGTZ+aJNHpTs2z9Amu37uGyuyfF6vAv9iT90OQVPDljdafyx6etYsnGndw7YVlxG4zpsWmruGHYHP4wZlHbcOGK1a4a6AuRNKbEpus3s/uBDwNHm9lKUqPBBgEPmdkVwDLgC2H1p4HzgUXALuByAHffbGY/ASaH9W5y9849y1IWha6neHzaSv77wRnplTspNNXO5277R9liytzVNaHJ7lPveFP27STUE75jT6qG1bR7vzrbRTIklmDc/Ys5XvpYlnUduCrHdu4G7i5jaN1SMSe/XIni/okrspbHtXrrnvgxhK/nXT1pJ91clT5W7u05t+JNZEpsUqN0JX9Ed29xKHSiitYmCp3TunrOK3SSLvazSuokHI0zXUuq1N9RLf+9Lli3nd+OWljtMKTKlGC6mXGLNrJ8U/4O71wnrnJPXhxrMEFXazAVOg17JP1We7BI34FPcXWW0XcvLNjAM7MSvj4puPiO8fxm1AJ27m2uyP6kNinBNJh12/awZ39Lp/L06e9Ld07kQ78cm3cbuU6QHTvxO5/5K3liLfbWCkm1IqWj6Djovvp1i8emdR5sOeDuSfzHfbmvcSqn9JB36d6UYCKq/c2zHM782Wi+/pfJhVfMonATWTHbSq29ZWd1Lz5Nf6bZfrffjFxQth1456KKqcXJLhvhf0m6TgkmohYv3ivFuMWbSvoHbxtmm7MGE103nh9kuSCzuJiyK8cJ7LejO/cRPDt7bcmJp62JrNj3ldgOWA83yKu91CeVlNgoMqktRY0iy3GKLHQizPa+HSW2wbfXPJz/un8aq5t288h/vK+kbUHhb/nn3voivQ89mBcWpC4U/e9PnFL8PtKd/DFP/GZW1Aezumk3I+as5fL3n1h0bJXW3nSoFNOdqQYTUQdfCBOVPhfMWrWV/xnWeS4xz7JuIaUe02iyGj5jNVOWbcl4PbtHp65k7Pz17evFDGDe2u1tySWO65+Yxf8Mm92hDyZ9SGaubIq9nWJ87c+T+PGTc1m/LWO4dxnP4dv37OfUG57hxSKORTb1ULuS5CnBdBNzVm8rav17xne+8r1QUukwZLeovZXP9x6eweV/7twH5Q4X3zGOvgOfKst+/jphOfeMXxb5nb3t+Hzngek8ncBora1hjrP0aL5sp/DNXezzmr92O3v2t2ZtPizF2370HCu3JDNNj9Q+JZhupNDtBApf2+JZl9Pmr93eqazUfq18nfPR1wtuJ7I8eemWnOuVKvr7RY/JaxvLc0vnzTv3MW7RxljrzljRxOk/GVmW/ZazaauYWxhIY1GCqWMTlmzi9ucXx17/58/My/t6oZNKc0v+1zOHR09csolJrxU3s0/6PjJJtrC4O9v2dG0Szc7bTOZizi/9aQJfunMiLa3eafvPzV0LwMZQa7nwD9mn4mluaY19C4FyiX586obpvpRg6tilQybwi2fzJ42org5DnhepoezZ38p3Hsg+lX56X5cMmcC+luKuh/jmX+NepxG3Iz0dT/tvd9/E5bz9R88VFVfh7WeMsivTWXXBuu2dtpfe54g561LrZKk5Rl3/xGzeP2hMWeKJrQaaS6X6lGAaVI5xYBWOouuizU6zVhbf1JLtOAx5cUnJ8exrbuW6xwvPBh0nv7TEmBrBslxnU6wHJsefQ64tYXZhf5lUg+m+lGC6kaT/0YvZ/AOTlvPYK7lv7ZOt7+Z3Y9o7nottQovGtr/IWlXUmHnruG9i9vvZdJjpoOQ9dNRhpoAcavJCy8hyLcYnlaEE06CuynLb44Kd+GU8DxRqIhr42CymLivc6d712ZTL05mzfNMu9uxv6TQfW1sCKHB0py3fQt+BTxVdCzugrQbjbTWeepiGJXrcVYPpvpRgGlS6fT4qetJ/PnKtSGSNBCMqTqGmmmL7ODr2j8R/33cfmMbY+ev50C/HcsmQCUzJGInWoQ8mx/4ARr2a+jw+9fuX2Z4xwCBvOJHtbwqd+be/EH9gRz77W1qzHEddvyLlowQT0egXh0VPJV/Lca1IrSj8SRTXyR9VTJPNE9NXt11XM2NFE3f/47WMKLJ/U8+3j4074l+rkq2JbPuejrMjuBOrNhi1v6WVftc9w81PvVrU+0pRQ39Wsaxq2s25t77I+u3x718k2SnBdCO5EsjYeeu5d/zSsu5r257Sp2mfs7q9GSlXv0bxfTDRmaBLjSzf9uPv45nZazr8jlG3PDuPX0fmQmuvyeXe4PrtexkxZ21R8ab7of46MfutpMt5o7dKTRezYnN5Lui8Z/xS5q3dzqNTc/cRSjxKMBGNXX/J3XZ/+V8mc8OwORWOJrcLBr9cttFM2T7Tsp7uIk1Y67btzbrK7FVbO5ywb3l2PhcMfjnrurc9v5jBkavos93ZM9sJu9iRcbnuGFqvlfgnZ6zmg7eM5aWFXZviRspLCaYbGb9kU4fnj0xdyZ8jTT5dPfFWcmLDpO9oOWLO2li/zy3hOqTMGkb62dRlW/jk714uut9k2vJUk1e2RFuOo5z4raQjy+X+s5i/djszVnSc7216eJ5tNgmpHiWYbuz7D8/gx0/ObXve1RNB0ullw/bsNYS8wpk019DiXP7vvVNjrZezPyUczHSzTbHH9h9hepissxJ7+ZJ5zkEUXdxuh1FkZf7LOOfWFzvNWtBa5EzWUhlKMBHd/W9zx96Oo5uai7xepLyHL7W1Pfvap5+ZvqKJG4fNZn9La6yLFHPFVImKlpO6MHRmCReHQvuJMv2zaVf7Z+M4K7eUaeqXShyLHPvoyvVIufbRzf+Fa44SjLQZ+GjHK9QzR0wVUs5zVTrZr97acSTP0PHLeO/Px3DRHePLuLfsikpEGesu37yLT/3+5aKPYVp6Wp70CfPSIRPaXmttbf/GXqo4gwfKJVuoM1Y00e+6ZzrcIsHdS75/ULpGd0ACGWbe2m0Mm64O/1IowUR0928/6zOaoEpqkqqAjTvix5V9Jt/yn1Qztzhs+uqitxH9Rv/kjNXcEem3iU5WWUxSmLK082SjZ/1sdO5O/rad5N7Hrn3NBSfP7Hglf2eTQ1zR67EenLyCt944gsUFZv3OJl2hPSCBDHPurS+1zbs3b+021m7V8OW4lGAkr1K/UXZVuU4TxczD1RWPT+v6N9zMocaDnpmX9UDErbw07dqX9b4+a7ftabvws5RU+9W7JhU1eWa2/qK2OdYiL6VjWry+lARTmT6Yc299ibN+PjrRfTQS3TK5Du3a11z0NPilMDN2dOF6lq7uOylxT9CVGhOXjidbv1LmRZUAryyPd1HlO2/KfW+Yb90Xd9bqzuJc1Fno5nP5+sYOKOGzz3cTtqKV+YOftnwL//qmwziwZ/f7Pt/9fuMG8K37XulwJX5Sdwx8aeHGst83Ja4k2tLT6u3K8kwbd+zjuw92vFVCqfk4s3ZRvsTevp2VWS6AzHYbBc94rTjpPpjy/eGUY1OLN+zgs7eN46dPzS28cgNSgolIov02Cc/P73gx2ZIN5bl7YqZX12zjP/82Lfb65RydFf++MMWr5PU6SZm2vON1ILX8lzt4zCKey2j+S8c7dPwymnalhnqnP5fvPDCd4TOK68NqDd1XtfAv3NrqtIYqVfoW1sXesrxRKMFElPPbTyUlGfb8ddW5cO3VNcn9Q27ZFa9WdtOTtTO7QSGl1jwcOPWGZ/jfEfM7lXctno7Pr7x3Kpf/eRJ3vrSEvgOf6jDMOj1CLr3PHXub+a/7432x2bprPxu27430weRe98HJy1m7dU/sWvmgZ+axfFPh1oEtO/dx66gFbUnlPTeP4oyfjeqwjpG6Jirzrq+NTn0wET1q4etPCXrUSGKcnnF1db0bmqWDvFbFvS4ok3vq7qS/H7uI75/zlpK2ce/4pbzv5KN5c69D2sqyjUAcO38DY0Pte2ZkdN+8Llx93//mkexvcS5693FA7kS7bNNOfhAZhj/6e//WId6o5six/N7D7U2RuZLD9U/M5qlZa3jXCUfwb6f0apv1Gtpr9XPXbOODt4wFYOmgC5i8dDM79jbzkbccE+O3rF9KMBH1mmBqun1EYpn02mYWlTB6qpzOvfVFBn3+7bHXHzx6Ib3eeFDbPHaX9D8ex/nxp99a8L3ZLrLM13K5eMMODuxxAMcf+U8Z20m9qdXz98Fk5t+P/eoF/nRZfz5xWm+WbdrJTU/O5cOnHsOhB/fkrpfbr12KJptcw6d37UsNxGhpzX3h6K59HZPTxeE6rqWDLsj5nqimXfuYu2Yb73vz0bHWrxVKMBG1UhPIpx5uNiXFmb6iiS/8MfkLRwuZt3Z71tsDrNu2h3vGL+UNB/Xklmfbm9Kisz4DPDglNSQ8291IM2UmmL9OWJb34tGP/eoFoOMJ+ZpHZrQtp996/ROzaG11vvCe4zu8v2eWL48/GqZHbNUAAAqoSURBVD6Hf79nStvz0fM63yMp2tcVDa+11Qv22X7+9nFF30Yhly/fOZE5q7ex6Obz6Nmjfno26ibBmNm5wG+BHsCd7j6o3PuohxrM+YNfqnYIUma1kFzy+e8HpzNu8abCKwbpRJPPlp0d+0Guf2J2p3Uee2Ul5721D68/sEdbWXSAxkNTVrYtp5PTnv2tXPPoTL7wnuNpbU1dkpprhoBCF4tm+updE9uWh7y0JHWdUkRLK/z07+2jxXIll62723/3acu38K4TjsDd+dNLS/jUO97E/ROX07/vkfx95mpuuegdQPsggXo4R0XVRYIxsx7AH4BPACuByWY23N3LOvavHj68bM0oizfs5M29DuGw17+uChFJI2p1Z+feZl7buLPDPGjlEufkfvVDM7j6oRkdys77bfYvWNlmTvjMbf8oeS64bKKDQzKTC9ChNpTPO378XNvyZ28bx//pcyjXnncqP3t6Hj97uuN2L3nPCR1qX997eAZnnXQUO/c2M+C9fTFLJdUWd95wYI+am+zT6mHIppm9F/iRu58Tnl8L4O4/z7Z+//79fcqUeB921Ki56/hGzD+SXE4+5pC2b1kdjqzTqSzbNQDZ7ooYLSvbJIciWbzhwB7s3Ne9Rjo1kn7HHMKH39KL6y44raT3m9lUd+9frnjqogYDHAtE690rgTOjK5jZlcCVACeccEJJO/noqV0b0dHvmEM4pfcbQ0AdfnT4ZtFeRpYy6/C847aMlVtWkmnwF9/F9j372bp7f4c2cpFinfPWf+axVzpOe9P70INy3kxNyuMTp/Vm5Nx1Jb33oJ4HsLe5ldP6HErfo/+J3oceXOboSlcvNZiLgHPd/Rvh+VeBM93929nWL7UGIyLSnZW7BlMvwxFWAdFhIceFMhERqVH1kmAmA/3M7EQzOxC4FBhe5ZhERCSPuuiDcfdmM/s2MILUMOW73b1+5vEQEemG6iLBALj708DT1Y5DRETiqZcmMhERqTNKMCIikgglGBERSYQSjIiIJKIuLrQslpltALpyM4+jgY1lCqfcFFtpajk2qO34FFtpajk2yB7fv7h7r3LtoCETTFeZ2ZRyXs1aToqtNLUcG9R2fIqtNLUcG1QmPjWRiYhIIpRgREQkEUow2Q2pdgB5KLbS1HJsUNvxKbbS1HJsUIH41AcjIiKJUA1GREQSoQQjIiLJcHc9wgM4F5gPLAIGJrif44GxwFxgDvCdUP4jUve5mR4e50fec22Iaz5wTqGYgROBiaH8QeDAIuJbCswKMUwJZUcCI4GF4ecRodyAwWE/M4HTI9sZENZfCAyIlL87bH9ReK/FjOstkWMzHdgGfLeaxw24G1gPzI6UJX6scu0jRmy/BOaF/T8OHB7K+wK7I8fwjlJjyPd7Fogt8c8ROCg8XxRe71vE5/pgJLalwPQqHbtc54+a+LvrEGu5Tpr1/iB1G4DFwEnAgcAM4LSE9tUn/SEDbwQWAKeFf7DvZ1n/tBDPQeEfZ3GIN2fMwEPApWH5DuA/iohvKXB0Rtkt6X9gYCDwi7B8PvBM+CM+C5gY+UNcEn4eEZbTf/CTwroW3nteiZ/XWuBfqnncgA8Bp9PxRJT4scq1jxixnQ30DMu/iMTWN7pexnaKiiHX7xkjtsQ/R+BbhARA6r5SD8b9XDNe/xXwP1U6drnOHzXxd9ch1mL/sRv1AbwXGBF5fi1wbYX2PQz4RJ5/sA6xkLovzntzxRz+KDbSfiLpsF6MeJbSOcHMB/qE5T7A/LD8R+CLmesBXwT+GCn/YyjrA8yLlHdYr4gYzwb+EZaretzIOMFU4ljl2keh2DJe+yxwX771Sokh1+8Z47gl/jmm3xuWe4b1stag8xwTA1YA/ap17DL2kz5/1MzfXfqhPph2x5L6o0lbGcoSZWZ9gXeRqq4DfNvMZprZ3WZ2RIHYcpUfBTS5e3NGeVwOPGdmU83sylDW293XhOW1QO8SYzs2LGeWF+tS4P7I81o4bmmVOFa59lGMr5P6dpp2oplNM7MXzOyDkZiLjaEr/0tJf45t7wmvbw3rF+ODwDp3Xxgpq8qxyzh/1NzfnRJMFZnZIcCjwHfdfRtwO/Bm4J3AGlLV8Gr4gLufDpwHXGVmH4q+6KmvL16VyIBw2+xPAw+Holo5bp1U4liVsg8zuw5oBu4LRWuAE9z9XcDVwN/M7NAkY8iiZj/HDF+k45ebqhy7LOePLm+zGHH2oQTTbhWpzrO040JZIszsdaT+OO5z98cA3H2du7e4eyvwJ+CMArHlKt8EHG5mPTPKY3H3VeHnelIdwWcA68ysT4i9D6kO0FJiWxWWM8uLcR7wiruvC3HWxHGLqMSxyrWPgszsa8AngS+HkwTuvtfdN4XlqaT6Nk4pMYaS/pcq9Dm2vSe8flhYP5bwns+R6vBPx13xY5ft/FHCNhP/u1OCaTcZ6GdmJ4ZvyJcCw5PYkZkZcBfwqrv/OlLeJ7LaZ4HZYXk4cKmZHWRmJwL9SHXCZY05nDTGAheF9w8g1U4bJ7Y3mNkb08uk+jpmhxgGZNnecOAySzkL2Bqq0COAs83siNDUcTapdvA1wDYzOysch8vixhbR4RtkLRy3DJU4Vrn2kZeZnQtcA3za3XdFynuZWY+wfBKpY7WkxBhy/Z6FYqvE5xiN+SJgTDrJxvRxUv0TbU1IlT52uc4fJWwz+b+7fB003e1BarTFAlLfQK5LcD8fIFW1nElkSCZwL6mhgTPDB9kn8p7rQlzziYy6yhUzqZE1k0gNM3wYOChmbCeRGo0zg9QQyOtC+VHAaFLDE0cBR4ZyA/4Q9j8L6B/Z1tfD/hcBl0fK+5M6eSwGfk/MYcrhvW8g9Y3zsEhZ1Y4bqUS3BthPqq36ikocq1z7iBHbIlLt7h2G1AKfD5/3dOAV4FOlxpDv9ywQW+KfI3BweL4ovH5S3M81lP8F+GbGupU+drnOHzXxdxd9aKoYERFJhJrIREQkEUowIiKSCCUYERFJhBKMiIgkQglGREQSoQQjUmFm9k0zu6yI9T9tZgNzvLajfJGJlJeGKYvUMTPb4e6HVDsOkWxUgxHJwcz6mtk8M/uLmS0ws/vM7ONm9g8zW2hmZ4T1jjSzJyw1SeMEM3u7mR1gZkvN7PDI9haaWW8z+5GZfT+UvdnMnrXUxKIvmdmpWeL4mpn9PiyfaGbjzWyWmf20UsdCpBRKMCL5nUxq0sVTw+NLpK6k/j7ww7DOj4Fp7v72UHaPp+bTGkZqyhPM7ExgmYf50yKGAP/p7u8O27ytQDy/BW5397eRutJcpGYpwYjk95q7zwoJYw4w2lPtyrNI3QcEUgnnXgB3HwMcZanZdB8ELgnrXEpkgkRomw33fcDDZjad1P04ovNxZfN+2udhu7cLv5dI4noWXkWkW9sbWW6NPG+l8P/PeOBkM+sFfAbIbNI6gNR9S95ZZEzqOJW6oBqMSNe9BHwZwMw+DGx0922hpvM48GtSM992mBbeU/fweM3MLg7vNTN7R4F9/YNUbYj0PkVqlRKMSNf9CHi3mc0EBtE+nTmkmsW+QkbzWMSXgSvMLD179YUF9vUdUjeBm0UF7rgq0hUapiwiIolQDUZERBKhBCMiIolQghERkUQowYiISCKUYEREJBFKMCIikgglGBERScT/B4vxyVUmxlWYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(movie_id_sort,num_of_rating)\n",
        "plt.xlabel('movie id')\n",
        "plt.ylabel('num of ratings')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z95xSozhNSV"
      },
      "source": [
        "Now, we obtain a data frame with movie id as one column and its number of ratings as another column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT13V6mnZTUA",
        "outputId": "48bd2323-e9a5-4565-c9a9-5c290fb998ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "item id\n",
              "2         1372\n",
              "3          776\n",
              "4          176\n",
              "5          794\n",
              "6         1404\n",
              "          ... \n",
              "193473       1\n",
              "193523       1\n",
              "193649       1\n",
              "193699       1\n",
              "193727       1\n",
              "Name: rating, Length: 22383, dtype: int64"
            ]
          },
          "execution_count": 86,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.groupby(\"item id\")['rating'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBvgoLA0hYiR"
      },
      "source": [
        "Our ratings dataset contains too many movies and users, we can trim the size of dataset by filtering observations with movies which have been rated less than a threshold value, we define a function *remove_movie* to remove some rows,and return a new dataframe and number of movies left after trimming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOarVxCzpoDU",
        "outputId": "0e2f8f38-e0d3-41de-ff3e-ec6e5673373d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4939"
            ]
          },
          "execution_count": 87,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.groupby(\"item id\")['rating'].count().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRwtub_7PX_d",
        "outputId": "58a37df5-dbbd-4f56-8305-7707d4bdb62b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3955"
            ]
          },
          "execution_count": 88,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#threshold should less than 996!\n",
        "def remove_movie(data,threshold):\n",
        "    series=data.groupby(\"item id\")['rating'].count()\n",
        "    movieid=np.array(series[series<threshold].index).tolist()#movie id need to be deleted\n",
        "    newdata=data[~data['item id'].isin(movieid)]\n",
        "    return newdata, newdata['item id'].nunique(),movieid\n",
        "rating,movie_size,movie_id_remove=remove_movie(data,50)\n",
        "movie_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7hDCA10WFoG"
      },
      "source": [
        "FIltering movies and tags DF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C4x5LQ9kmoc"
      },
      "outputs": [],
      "source": [
        "movies=movies[~movies['movie id'].isin(movie_id_remove)]\n",
        "tags=tags[~tags['item id'].isin(movie_id_remove)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "6t5_njwxlk1o",
        "outputId": "03e2153f-4b2b-4a5a-b0c1-a7d9544b2e68"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user id</th>\n",
              "      <th>item id</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>42</td>\n",
              "      <td>37733</td>\n",
              "      <td>disappointing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>42</td>\n",
              "      <td>37733</td>\n",
              "      <td>overrated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>42</td>\n",
              "      <td>37733</td>\n",
              "      <td>stupid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>332</td>\n",
              "      <td>236</td>\n",
              "      <td>Meg Ryan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>332</td>\n",
              "      <td>3408</td>\n",
              "      <td>Julia Roberts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108436</th>\n",
              "      <td>282989</td>\n",
              "      <td>2571</td>\n",
              "      <td>martial arts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108437</th>\n",
              "      <td>282989</td>\n",
              "      <td>2571</td>\n",
              "      <td>ulhas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108438</th>\n",
              "      <td>282989</td>\n",
              "      <td>6365</td>\n",
              "      <td>awesome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108672</th>\n",
              "      <td>283117</td>\n",
              "      <td>4011</td>\n",
              "      <td>Jason Statham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108673</th>\n",
              "      <td>283117</td>\n",
              "      <td>4011</td>\n",
              "      <td>twist ending</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36613 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         user id  item id            tag\n",
              "16            42    37733  disappointing\n",
              "17            42    37733      overrated\n",
              "18            42    37733         stupid\n",
              "386          332      236       Meg Ryan\n",
              "387          332     3408  Julia Roberts\n",
              "...          ...      ...            ...\n",
              "1108436   282989     2571   martial arts\n",
              "1108437   282989     2571          ulhas\n",
              "1108438   282989     6365        awesome\n",
              "1108672   283117     4011  Jason Statham\n",
              "1108673   283117     4011   twist ending\n",
              "\n",
              "[36613 rows x 3 columns]"
            ]
          },
          "execution_count": 90,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chP0R2eOiGx5"
      },
      "source": [
        "Now we trim the size of our dataframe to include 8156 movies which has been rated by users at least 200 times!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqWBS_hSzJPY",
        "outputId": "db4c1be4-67c2-47cc-8c7f-8c0195a40335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pandas.core.groupby.generic.SeriesGroupBy object at 0x7fc770150710>"
            ]
          },
          "execution_count": 91,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating.groupby('item id')['user id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfFvjUpzjJi4"
      },
      "source": [
        "#rating data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "4n7D98YpiNrD",
        "outputId": "d3627841-9aa2-4b87-ab72-bf538f7e40a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1250207, 3)\n",
            "3955\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user id</th>\n",
              "      <th>item id</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9338781</th>\n",
              "      <td>96225</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16432694</th>\n",
              "      <td>168057</td>\n",
              "      <td>2</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5852589</th>\n",
              "      <td>60233</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16454220</th>\n",
              "      <td>168236</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5842735</th>\n",
              "      <td>60108</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17780969</th>\n",
              "      <td>181785</td>\n",
              "      <td>187593</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15116819</th>\n",
              "      <td>154497</td>\n",
              "      <td>187593</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7878187</th>\n",
              "      <td>81037</td>\n",
              "      <td>187593</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25956274</th>\n",
              "      <td>265010</td>\n",
              "      <td>187593</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9794245</th>\n",
              "      <td>100983</td>\n",
              "      <td>187593</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1250207 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          user id  item id  rating\n",
              "9338781     96225        2     3.0\n",
              "16432694   168057        2     4.0\n",
              "5852589     60233        2     3.0\n",
              "16454220   168236        2     3.0\n",
              "5842735     60108        2     2.0\n",
              "...           ...      ...     ...\n",
              "17780969   181785   187593     4.0\n",
              "15116819   154497   187593     2.5\n",
              "7878187     81037   187593     4.0\n",
              "25956274   265010   187593     3.5\n",
              "9794245    100983   187593     4.0\n",
              "\n",
              "[1250207 rows x 3 columns]"
            ]
          },
          "execution_count": 92,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(rating.shape)\n",
        "print(rating['item id'].nunique())\n",
        "rating.sort_values(by='item id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aUWvuNYZJq5"
      },
      "source": [
        "Create movie-user matrix:very sparse matrix, most entries are 0's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGvh6ZVcDTKj"
      },
      "outputs": [],
      "source": [
        "movie_user_mat =rating.pivot(index='item id',columns='user id',values='rating').fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "lGkKoGqX4Bi_",
        "outputId": "b14163d1-fa40-4bbb-afa2-74c242cfc041"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>user id</th>\n",
              "      <th>17</th>\n",
              "      <th>19</th>\n",
              "      <th>33</th>\n",
              "      <th>42</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>124</th>\n",
              "      <th>162</th>\n",
              "      <th>179</th>\n",
              "      <th>182</th>\n",
              "      <th>188</th>\n",
              "      <th>215</th>\n",
              "      <th>294</th>\n",
              "      <th>303</th>\n",
              "      <th>332</th>\n",
              "      <th>336</th>\n",
              "      <th>346</th>\n",
              "      <th>371</th>\n",
              "      <th>391</th>\n",
              "      <th>397</th>\n",
              "      <th>416</th>\n",
              "      <th>420</th>\n",
              "      <th>443</th>\n",
              "      <th>446</th>\n",
              "      <th>526</th>\n",
              "      <th>571</th>\n",
              "      <th>595</th>\n",
              "      <th>613</th>\n",
              "      <th>630</th>\n",
              "      <th>635</th>\n",
              "      <th>648</th>\n",
              "      <th>651</th>\n",
              "      <th>670</th>\n",
              "      <th>684</th>\n",
              "      <th>724</th>\n",
              "      <th>753</th>\n",
              "      <th>757</th>\n",
              "      <th>772</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>...</th>\n",
              "      <th>282275</th>\n",
              "      <th>282316</th>\n",
              "      <th>282367</th>\n",
              "      <th>282378</th>\n",
              "      <th>282399</th>\n",
              "      <th>282419</th>\n",
              "      <th>282420</th>\n",
              "      <th>282435</th>\n",
              "      <th>282454</th>\n",
              "      <th>282469</th>\n",
              "      <th>282548</th>\n",
              "      <th>282587</th>\n",
              "      <th>282650</th>\n",
              "      <th>282667</th>\n",
              "      <th>282671</th>\n",
              "      <th>282679</th>\n",
              "      <th>282685</th>\n",
              "      <th>282695</th>\n",
              "      <th>282723</th>\n",
              "      <th>282726</th>\n",
              "      <th>282727</th>\n",
              "      <th>282780</th>\n",
              "      <th>282783</th>\n",
              "      <th>282853</th>\n",
              "      <th>282858</th>\n",
              "      <th>282862</th>\n",
              "      <th>282877</th>\n",
              "      <th>282886</th>\n",
              "      <th>282904</th>\n",
              "      <th>282926</th>\n",
              "      <th>282931</th>\n",
              "      <th>282944</th>\n",
              "      <th>282951</th>\n",
              "      <th>282955</th>\n",
              "      <th>282977</th>\n",
              "      <th>282989</th>\n",
              "      <th>283019</th>\n",
              "      <th>283060</th>\n",
              "      <th>283106</th>\n",
              "      <th>283117</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177765</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179819</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180031</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185029</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187593</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3955 rows × 14143 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "user id  17      19      33      42      ...  283019  283060  283106  283117\n",
              "item id                                  ...                                \n",
              "2           0.0     0.0     0.0     3.0  ...     0.0     0.0     0.0     0.0\n",
              "3           0.0     4.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "4           0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "5           0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "6           0.0     4.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "...         ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "177765      0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "179819      0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "180031      0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "185029      0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "187593      0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0     0.0\n",
              "\n",
              "[3955 rows x 14143 columns]"
            ]
          },
          "execution_count": 94,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_user_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMNuIkffRdjI"
      },
      "source": [
        "After filtering, how may movies and users left?\n",
        "There are *3920* movies left, *14144* users left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blaPU3rgRVih",
        "outputId": "1af06593-c90f-419f-f496-a4552b18ece1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3955, 14143)\n"
          ]
        }
      ],
      "source": [
        "print(movie_user_mat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdetc6HR5-SH"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "model_knn = NearestNeighbors(metric='jaccard', algorithm='auto', n_neighbors=20, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwpj8FJllLil"
      },
      "source": [
        "Hashmap is a dictionary with key as movie title, value as index from 0 to the length of movies. These movies are selected in rating data frame after filtering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdEIT9wvIklz"
      },
      "outputs": [],
      "source": [
        "# create mapper from movie title to index\n",
        "hashmap={}\n",
        "for i in movie_user_mat.index:\n",
        "     movie=movies.set_index('movie id').loc[i].title\n",
        "     hashmap[movie]=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0mj3NNnoZLd",
        "outputId": "88e6a6e0-3354-4f0d-f82a-b1dc7a8d3c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt4Umzhmn9sX",
        "outputId": "887755de-dbab-4827-cdc2-53273d4c47c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "from fuzzywuzzy import fuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzu0sVVWlkLs"
      },
      "outputs": [],
      "source": [
        "def fuzzy_matching(hashmap, fav_movie):\n",
        "        # \"\"\"\n",
        "        # return the closest match via fuzzy ratio.\n",
        "        # If no match found, return None\n",
        "        # Parameters\n",
        "        # ----------\n",
        "        # hashmap: dict, map movie title name to index of the movie in data\n",
        "        # fav_movie: str, name of user input movie\n",
        "        # Return\n",
        "        # ------\n",
        "        # index of the closest match\n",
        "        # \"\"\"\n",
        "        match_tuple = []\n",
        "        # get match\n",
        "        for title, idx in hashmap.items():\n",
        "            ratio = fuzz.ratio(title.lower(), fav_movie.lower())\n",
        "            if ratio >= 60:\n",
        "                match_tuple.append((title, idx, ratio))\n",
        "        # sort\n",
        "        match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]\n",
        "        if not match_tuple:\n",
        "            print('Oops! No match is found')\n",
        "        else:\n",
        "            print('Found possible matches in our database: '\n",
        "                  '{0}\\n'.format([x[0] for x in match_tuple]))\n",
        "            return match_tuple[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFiIRWeuDZiB"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IwgLqYI0cSy",
        "outputId": "10257067-8fca-48f9-a437-87966e13b4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found possible matches in our database: ['Jumanji (1995)', 'Junior (1994)', 'Darkman (1990)', 'Jade (1995)', 'Mermaids (1990)', 'Magnolia (1999)', 'Superman II (1980)', 'Mulan (1998)', 'Dead Man (1995)', 'Angus (1995)', 'Nixon (1995)', 'It (1990)', 'Tank Girl (1995)', 'My Family (1995)']\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 99,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Test:\n",
        "fuzzy_matching(hashmap, 'Jumanji (19950')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5D3FWznTXd8"
      },
      "outputs": [],
      "source": [
        "reverse_hashmap = {v: k for k, v in hashmap.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwsmFZWJlkSr"
      },
      "outputs": [],
      "source": [
        "def inference(model, data, hashmap,\n",
        "                   fav_movie, n_recommendations):\n",
        "        # \"\"\"\n",
        "        # return top n similar movie recommendations based on user's input movie\n",
        "        # Parameters\n",
        "        # ----------\n",
        "        # model: sklearn model, knn model\n",
        "        # data: movie-user matrix\n",
        "        # hashmap: dict, map movie title name to index of the movie in data\n",
        "        # fav_movie: str, name of user input movie\n",
        "        # n_recommendations: int, top n recommendations\n",
        "        # Return\n",
        "        # ------\n",
        "        # list of top n similar movie recommendations\n",
        "        # \"\"\"\n",
        "        # fit\n",
        "        model.fit(np.array(data))\n",
        "        # get input movie index\n",
        "        print('You have input movie:', fav_movie)\n",
        "        idx = fuzzy_matching(hashmap, fav_movie)\n",
        "        # inference\n",
        "        print('Recommendation system start to make inference')\n",
        "        print('......\\n')\n",
        "        t0 = time.time()\n",
        "        distances, indices = model.kneighbors(\n",
        "            np.array(data.loc[idx]).reshape(1,-1),\n",
        "            n_neighbors=n_recommendations+1)\n",
        "        indices=data.index[indices]\n",
        "        # get list of raw idx of recommendations\n",
        "        raw_recommends = \\\n",
        "            sorted(\n",
        "                list(\n",
        "                    zip(\n",
        "                        indices.squeeze().tolist(),\n",
        "                        distances.squeeze().tolist()\n",
        "                    )\n",
        "                ),\n",
        "                key=lambda x: x[1]\n",
        "            ,reverse=True)[:0:-1]\n",
        "        print('It took my system {:.2f}s to make inference \\n\\\n",
        "              '.format(time.time() - t0))\n",
        "        # return recommendation (movieId, distance)\n",
        "        return raw_recommends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NReabSo1Rjzo"
      },
      "outputs": [],
      "source": [
        "def make_recommendations(fav_movie,model,data, n_recommendations):\n",
        "        # \"\"\"\n",
        "        # make top n movie recommendations\n",
        "        # Parameters\n",
        "        # ----------\n",
        "        # fav_movie: str, name of user input movie\n",
        "        # n_recommendations: int, top n recommendations\n",
        "        # \"\"\"\n",
        "        # get recommendations\n",
        "        raw_recommends = inference(\n",
        "            model, data, hashmap,\n",
        "            fav_movie, n_recommendations)\n",
        "        # print results\n",
        "        reverse_hashmap = {v: k for k, v in hashmap.items()}\n",
        "        print('Recommendations for {}:'.format(fav_movie))\n",
        "        for i,(idx, dist) in enumerate(raw_recommends):\n",
        "            print('{0}: {1}, with distance '\n",
        "                  'of {2}'.format(i+1,reverse_hashmap[idx], dist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDgMMx63TZNC",
        "outputId": "01083ea4-dd18-4b94-8239-dfabd9049e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have input movie: Tommy Boy (1995)\n",
            "Found possible matches in our database: ['Tommy Boy (1995)', 'Tommy (1975)', 'Rob Roy (1995)', 'Toy Story 2 (1999)', 'Bad Boys (1995)', 'Sommersby (1993)', 'Tombstone (1993)', 'My Family (1995)', 'Toys (1992)', 'To Die For (1995)', 'Get Shorty (1995)', 'Commando (1985)', 'Only You (1994)', 'Miami Rhapsody (1995)', 'Mad Love (1995)', 'Smoke (1995)', 'Congo (1995)', 'Balto (1995)', 'My Bodyguard (1980)', 'Jerky Boys, The (1995)', 'Tom and Huck (1995)', 'Rio Bravo (1959)', 'Drugstore Cowboy (1989)', '8MM (1999)', 'Tom Jones (1963)', 'Cabin Boy (1994)', 'Jury Duty (1995)', 'Cat Ballou (1965)', 'Mummy, The (1999)', 'Dirty Work (1998)', 'Roman Holiday (1953)', 'Sgt. Bilko (1996)', 'Home Alone (1990)', 'Virtuosity (1995)', 'Milk Money (1994)', 'Party Girl (1995)', 'Two if by Sea (1996)', 'Mortal Kombat (1995)', 'Tremors (1990)', 'My Girl (1991)', 'Tampopo (1985)', 'Mumford (1999)', 'Stepmom (1998)', 'Rocky V (1990)', 'Orgazmo (1997)', 'Top Hat (1935)', 'Timecop (1994)', 'Othello (1995)', 'Copycat (1995)']\n",
            "\n",
            "Recommendation system start to make inference\n",
            "......\n",
            "\n",
            "It took my system 0.20s to make inference \n",
            "              \n",
            "Recommendations for Tommy Boy (1995):\n",
            "1: Tommy Boy (1995), with distance of 0.0\n",
            "2: Billy Madison (1995), with distance of 0.7263948497854077\n",
            "3: Coneheads (1993), with distance of 0.766923736075407\n",
            "4: So I Married an Axe Murderer (1993), with distance of 0.7856502242152467\n",
            "5: Brady Bunch Movie, The (1995), with distance of 0.7888888888888889\n",
            "6: Santa Clause, The (1994), with distance of 0.7900552486187845\n",
            "7: City Slickers II: The Legend of Curly's Gold (1994), with distance of 0.7979274611398963\n",
            "8: Kingpin (1996), with distance of 0.8001850138760407\n",
            "9: Robin Hood: Men in Tights (1993), with distance of 0.8003182179793158\n",
            "10: Happy Gilmore (1996), with distance of 0.8024357239512855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
          ]
        }
      ],
      "source": [
        "#Test case 1:\n",
        "make_recommendations('Tommy Boy (1995)',model_knn,movie_user_mat,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n533C8zIBGLe",
        "outputId": "c567db38-127d-4411-91ce-c051f4ba01f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have input movie: Sense and Sensibility (1995)\n",
            "Found possible matches in our database: ['Sense and Sensibility (1995)', 'Angels and Insects (1995)', 'Now and Then (1995)', 'Dead Presidents (1995)', 'Sudden Death (1995)']\n",
            "\n",
            "Recommendation system start to make inference\n",
            "......\n",
            "\n",
            "It took my system 0.20s to make inference \n",
            "              \n",
            "Recommendations for Sense and Sensibility (1995):\n",
            "1: Sense and Sensibility (1995), with distance of 0.0\n",
            "2: Leaving Las Vegas (1995), with distance of 0.7413185707096125\n",
            "3: Four Weddings and a Funeral (1994), with distance of 0.7424583520936515\n",
            "4: Dead Man Walking (1995), with distance of 0.7484817813765182\n",
            "5: Birdcage, The (1996), with distance of 0.7596861206473762\n",
            "6: Mr. Holland's Opus (1995), with distance of 0.7615894039735099\n",
            "7: Truth About Cats & Dogs, The (1996), with distance of 0.7732018561484919\n",
            "8: Piano, The (1993), with distance of 0.7773459189339256\n",
            "9: Emma (1996), with distance of 0.7805049088359046\n",
            "10: Sleepless in Seattle (1993), with distance of 0.780948384389425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
          ]
        }
      ],
      "source": [
        "#Test case 2:\n",
        "make_recommendations('Sense and Sensibility (1995)',model_knn,movie_user_mat,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_bYSQTUATvv"
      },
      "source": [
        "#2) Auto-encoder:#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy0O4xKEh1OP"
      },
      "outputs": [],
      "source": [
        "# Importing tensorflow\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "# Importing some more libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50i3_lydItQK"
      },
      "outputs": [],
      "source": [
        "ratings_pivot=rating.pivot(index='user id',columns='item id',values='rating').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcUHKCW4h1SY"
      },
      "outputs": [],
      "source": [
        "# creating train and test sets\n",
        "X_train, X_test = train_test_split(ratings_pivot, train_size=0.8)\n",
        "X_train, X_val = train_test_split(X_train, train_size=0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GOawzGjWIe",
        "outputId": "68328643-367a-43b8-be66-63d379e15324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9049, 4059)\n",
            "(2263, 4059)\n",
            "(2828, 4059)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkSY4mKgh1WI"
      },
      "outputs": [],
      "source": [
        "# Deciding how many nodes each layer should have\n",
        "n_nodes_inpl = X_train.shape[1]\n",
        "n_nodes_hl1  = 256\n",
        "n_nodes_outl = X_train.shape[1]\n",
        "# first hidden layer has 784*32 weights and 32 biases\n",
        "hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_inpl+1,n_nodes_hl1]))}\n",
        "# first hidden layer has 784*32 weights and 32 biases\n",
        "output_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1+1,n_nodes_outl])) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzHDGLr-h1Ya",
        "outputId": "b126ecb6-2c05-4509-e45c-ce7c52498ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "# user with 3706 ratings goes in\n",
        "input_layer = tf.placeholder('float', [None, X_train.shape[1]])\n",
        "# add a constant node to the first layer\n",
        "# it needs to have the same shape as the input layer for me to be\n",
        "# able to concatinate it later\n",
        "input_layer_const = tf.fill( [tf.shape(input_layer)[0], 1] ,1.0  )\n",
        "input_layer_concat =  tf.concat([input_layer, input_layer_const], 1)\n",
        "# multiply output of input_layer wth a weight matrix\n",
        "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat,\\\n",
        "hidden_1_layer_vals['weights']))\n",
        "# adding one bias node to the hidden layer\n",
        "layer1_const = tf.fill( [tf.shape(layer_1)[0], 1] ,1.0  )\n",
        "layer_concat =  tf.concat([layer_1, layer1_const], 1)\n",
        "# multiply output of hidden with a weight matrix to get final output\n",
        "output_layer = tf.matmul( layer_concat,output_layer_vals['weights'])\n",
        "# output_true shall have the original shape for error calculations\n",
        "output_true = tf.placeholder('float', [None, X_train.shape[1]])\n",
        "# define our cost function\n",
        "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
        "# define our optimizer\n",
        "learn_rate = 0.1   # how fast the model should learn\n",
        "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB0nYkFOd25I"
      },
      "outputs": [],
      "source": [
        "# initialising variables and starting the session\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "# defining batch size, number of epochs and learning rate\n",
        "batch_size = 100  # how many images to use together for training\n",
        "hm_epochs =400    # how many times to go through the entire dataset\n",
        "tot_users = X_train.shape[0] # total number of users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCfpt2VaeQu8",
        "outputId": "36c30933-6139-4e64-b207-ee53df77849e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE train 57.48598447717692 MSE test 57.675476586004756\n",
            "Epoch 0 / 400 loss: 7032.75662612915\n",
            "MSE train 43.59329427578469 MSE test 43.691861198751276\n",
            "Epoch 1 / 400 loss: 4489.733005523682\n",
            "MSE train 35.46791325129852 MSE test 35.55757948912769\n",
            "Epoch 2 / 400 loss: 3548.1943893432617\n",
            "MSE train 29.9598050379521 MSE test 30.114151473657298\n",
            "Epoch 3 / 400 loss: 2941.6142539978027\n",
            "MSE train 26.32395293099139 MSE test 26.492072871761852\n",
            "Epoch 4 / 400 loss: 2534.2001991271973\n",
            "MSE train 23.70049628419113 MSE test 23.918680755177284\n",
            "Epoch 5 / 400 loss: 2256.2596397399902\n",
            "MSE train 21.805843913101803 MSE test 22.07401115529058\n",
            "Epoch 6 / 400 loss: 2053.9328689575195\n",
            "MSE train 20.3246911265242 MSE test 20.637284682667968\n",
            "Epoch 7 / 400 loss: 1902.3302822113037\n",
            "MSE train 19.111178324180962 MSE test 19.468428197107684\n",
            "Epoch 8 / 400 loss: 1780.9461431503296\n",
            "MSE train 18.10216325788671 MSE test 18.486086898233303\n",
            "Epoch 9 / 400 loss: 1680.8235416412354\n",
            "MSE train 17.220455724721177 MSE test 17.629837173384253\n",
            "Epoch 10 / 400 loss: 1595.471158027649\n",
            "MSE train 16.45574269586965 MSE test 16.895955165950124\n",
            "Epoch 11 / 400 loss: 1520.9308815002441\n",
            "MSE train 15.775260874304385 MSE test 16.259284453884096\n",
            "Epoch 12 / 400 loss: 1455.6468887329102\n",
            "MSE train 15.170819692661208 MSE test 15.68593779400778\n",
            "Epoch 13 / 400 loss: 1397.4989347457886\n",
            "MSE train 14.626592053783392 MSE test 15.165731114821064\n",
            "Epoch 14 / 400 loss: 1345.5081310272217\n",
            "MSE train 14.13072636319495 MSE test 14.690088986043806\n",
            "Epoch 15 / 400 loss: 1298.3928127288818\n",
            "MSE train 13.676369947954841 MSE test 14.25275959926837\n",
            "Epoch 16 / 400 loss: 1255.471261024475\n",
            "MSE train 13.250127991269173 MSE test 13.843837966673965\n",
            "Epoch 17 / 400 loss: 1215.6486587524414\n",
            "MSE train 12.851534218593121 MSE test 13.458852032657212\n",
            "Epoch 18 / 400 loss: 1178.3333892822266\n",
            "MSE train 12.480446544577724 MSE test 13.098599730666534\n",
            "Epoch 19 / 400 loss: 1143.4463138580322\n",
            "MSE train 12.133189527905282 MSE test 12.762051539877564\n",
            "Epoch 20 / 400 loss: 1110.9338092803955\n",
            "MSE train 11.805862376371596 MSE test 12.448216792767983\n",
            "Epoch 21 / 400 loss: 1080.4205446243286\n",
            "MSE train 11.496121367642761 MSE test 12.154290100637146\n",
            "Epoch 22 / 400 loss: 1051.6279296875\n",
            "MSE train 11.198084838928619 MSE test 11.873084186936685\n",
            "Epoch 23 / 400 loss: 1024.1722202301025\n",
            "MSE train 10.914854954732471 MSE test 11.603935538670946\n",
            "Epoch 24 / 400 loss: 997.8956422805786\n",
            "MSE train 10.644036876924396 MSE test 11.34652823325213\n",
            "Epoch 25 / 400 loss: 972.8345260620117\n",
            "MSE train 10.38545448532753 MSE test 11.101057916118462\n",
            "Epoch 26 / 400 loss: 948.8952898979187\n",
            "MSE train 10.139102575350991 MSE test 10.867094263667889\n",
            "Epoch 27 / 400 loss: 926.0300483703613\n",
            "MSE train 9.904642283942428 MSE test 10.643749121143237\n",
            "Epoch 28 / 400 loss: 904.2750959396362\n",
            "MSE train 9.679847307261953 MSE test 10.428932981002312\n",
            "Epoch 29 / 400 loss: 883.523877620697\n",
            "MSE train 9.464548604110107 MSE test 10.222574905774097\n",
            "Epoch 30 / 400 loss: 863.6182708740234\n",
            "MSE train 9.25792013786016 MSE test 10.02423297854387\n",
            "Epoch 31 / 400 loss: 844.5556893348694\n",
            "MSE train 9.057925039889588 MSE test 9.83222648236282\n",
            "Epoch 32 / 400 loss: 826.1974081993103\n",
            "MSE train 8.866622564282052 MSE test 9.647706767436391\n",
            "Epoch 33 / 400 loss: 808.4896945953369\n",
            "MSE train 8.683241319271648 MSE test 9.469915345206372\n",
            "Epoch 34 / 400 loss: 791.5548338890076\n",
            "MSE train 8.506614348448982 MSE test 9.298919413291852\n",
            "Epoch 35 / 400 loss: 775.2807354927063\n",
            "MSE train 8.336633868459204 MSE test 9.134916091428108\n",
            "Epoch 36 / 400 loss: 759.6184945106506\n",
            "MSE train 8.173257405490874 MSE test 8.977401788700726\n",
            "Epoch 37 / 400 loss: 744.5276436805725\n",
            "MSE train 8.015808221813465 MSE test 8.82623753604462\n",
            "Epoch 38 / 400 loss: 730.0333337783813\n",
            "MSE train 7.864047624645929 MSE test 8.6805655188946\n",
            "Epoch 39 / 400 loss: 716.052116394043\n",
            "MSE train 7.717520277045267 MSE test 8.539629871213085\n",
            "Epoch 40 / 400 loss: 702.5755453109741\n",
            "MSE train 7.575099031096196 MSE test 8.402980370825748\n",
            "Epoch 41 / 400 loss: 689.5373682975769\n",
            "MSE train 7.436473419742806 MSE test 8.270176164484628\n",
            "Epoch 42 / 400 loss: 676.8459796905518\n",
            "MSE train 7.3015371343708 MSE test 8.141067418813552\n",
            "Epoch 43 / 400 loss: 664.4968509674072\n",
            "MSE train 7.17090927530993 MSE test 8.01577105300005\n",
            "Epoch 44 / 400 loss: 652.4923024177551\n",
            "MSE train 7.043916460562587 MSE test 7.8940304156390635\n",
            "Epoch 45 / 400 loss: 640.8558058738708\n",
            "MSE train 6.920801928619059 MSE test 7.776034934579533\n",
            "Epoch 46 / 400 loss: 629.5480551719666\n",
            "MSE train 6.801372127078106 MSE test 7.661423788082195\n",
            "Epoch 47 / 400 loss: 618.5851273536682\n",
            "MSE train 6.6852127912280555 MSE test 7.549828755637526\n",
            "Epoch 48 / 400 loss: 607.9409546852112\n",
            "MSE train 6.572659863808216 MSE test 7.441152844287651\n",
            "Epoch 49 / 400 loss: 597.5963544845581\n",
            "MSE train 6.46367404226129 MSE test 7.335469328200556\n",
            "Epoch 50 / 400 loss: 587.5752606391907\n",
            "MSE train 6.357530096097041 MSE test 7.232585609992615\n",
            "Epoch 51 / 400 loss: 577.862922668457\n",
            "MSE train 6.2541202105451115 MSE test 7.132304103197783\n",
            "Epoch 52 / 400 loss: 568.3964653015137\n",
            "MSE train 6.153790181234159 MSE test 7.034799990624965\n",
            "Epoch 53 / 400 loss: 559.1819648742676\n",
            "MSE train 6.0561809289228075 MSE test 6.940140066625204\n",
            "Epoch 54 / 400 loss: 550.243082523346\n",
            "MSE train 5.96109430487942 MSE test 6.848106619606384\n",
            "Epoch 55 / 400 loss: 541.5399293899536\n",
            "MSE train 5.868022515846118 MSE test 6.75854565300439\n",
            "Epoch 56 / 400 loss: 533.0545678138733\n",
            "MSE train 5.776964527165342 MSE test 6.671301234537659\n",
            "Epoch 57 / 400 loss: 524.7474553585052\n",
            "MSE train 5.688714834985023 MSE test 6.586465241234799\n",
            "Epoch 58 / 400 loss: 516.6364715099335\n",
            "MSE train 5.603269314712807 MSE test 6.504051394461453\n",
            "Epoch 59 / 400 loss: 508.78066658973694\n",
            "MSE train 5.520497234248625 MSE test 6.423972848767018\n",
            "Epoch 60 / 400 loss: 501.17525815963745\n",
            "MSE train 5.4402862639659775 MSE test 6.346160601008063\n",
            "Epoch 61 / 400 loss: 493.80445194244385\n",
            "MSE train 5.362133809386676 MSE test 6.2703375165488415\n",
            "Epoch 62 / 400 loss: 486.65639328956604\n",
            "MSE train 5.286155889984105 MSE test 6.196361989585079\n",
            "Epoch 63 / 400 loss: 479.69350576400757\n",
            "MSE train 5.211863281981158 MSE test 6.12405487692997\n",
            "Epoch 64 / 400 loss: 472.92031502723694\n",
            "MSE train 5.139068508651344 MSE test 6.053319588389758\n",
            "Epoch 65 / 400 loss: 466.2926778793335\n",
            "MSE train 5.06789169837245 MSE test 5.9841655507654625\n",
            "Epoch 66 / 400 loss: 459.79588174819946\n",
            "MSE train 4.998577974733626 MSE test 5.9166819744387285\n",
            "Epoch 67 / 400 loss: 453.4500505924225\n",
            "MSE train 4.930920454641825 MSE test 5.850708472134245\n",
            "Epoch 68 / 400 loss: 447.26573967933655\n",
            "MSE train 4.86508057664671 MSE test 5.786270846880679\n",
            "Epoch 69 / 400 loss: 441.2324070930481\n",
            "MSE train 4.800666671144337 MSE test 5.723257610629394\n",
            "Epoch 70 / 400 loss: 435.35876202583313\n",
            "MSE train 4.7376005569777755 MSE test 5.661669910974158\n",
            "Epoch 71 / 400 loss: 429.609087228775\n",
            "MSE train 4.676148986154746 MSE test 5.601607092553582\n",
            "Epoch 72 / 400 loss: 423.98265314102173\n",
            "MSE train 4.616197723658644 MSE test 5.543026414731285\n",
            "Epoch 73 / 400 loss: 418.50154972076416\n",
            "MSE train 4.557526046889101 MSE test 5.485809182108284\n",
            "Epoch 74 / 400 loss: 413.150595664978\n",
            "MSE train 4.499944646224086 MSE test 5.429853937276912\n",
            "Epoch 75 / 400 loss: 407.9100728034973\n",
            "MSE train 4.4436707630022845 MSE test 5.375173078391512\n",
            "Epoch 76 / 400 loss: 402.7684233188629\n",
            "MSE train 4.388782842300512 MSE test 5.321780135210277\n",
            "Epoch 77 / 400 loss: 397.7460627555847\n",
            "MSE train 4.33528807740871 MSE test 5.269663009920423\n",
            "Epoch 78 / 400 loss: 392.8470513820648\n",
            "MSE train 4.28333376358042 MSE test 5.218842649682861\n",
            "Epoch 79 / 400 loss: 388.07497239112854\n",
            "MSE train 4.232610085292626 MSE test 5.169182181613229\n",
            "Epoch 80 / 400 loss: 383.4378352165222\n",
            "MSE train 4.183264173239432 MSE test 5.1206858049152775\n",
            "Epoch 81 / 400 loss: 378.9131841659546\n",
            "MSE train 4.135044666246686 MSE test 5.073297917209093\n",
            "Epoch 82 / 400 loss: 374.50992465019226\n",
            "MSE train 4.087891259798997 MSE test 5.026950991108373\n",
            "Epoch 83 / 400 loss: 370.20401906967163\n",
            "MSE train 4.041889574596826 MSE test 4.981676675092189\n",
            "Epoch 84 / 400 loss: 365.99416184425354\n",
            "MSE train 3.996886315326294 MSE test 4.937365759204879\n",
            "Epoch 85 / 400 loss: 361.8854157924652\n",
            "MSE train 3.9527700853439414 MSE test 4.893952007764975\n",
            "Epoch 86 / 400 loss: 357.86536502838135\n",
            "MSE train 3.9095469808056587 MSE test 4.851375138665484\n",
            "Epoch 87 / 400 loss: 353.92274618148804\n",
            "MSE train 3.8673284789906397 MSE test 4.809651979870488\n",
            "Epoch 88 / 400 loss: 350.0622103214264\n",
            "MSE train 3.8259123213707538 MSE test 4.7687420062600685\n",
            "Epoch 89 / 400 loss: 346.2895770072937\n",
            "MSE train 3.7854392445740603 MSE test 4.728688790891469\n",
            "Epoch 90 / 400 loss: 342.589613199234\n",
            "MSE train 3.745750938329099 MSE test 4.689479898880339\n",
            "Epoch 91 / 400 loss: 338.97320914268494\n",
            "MSE train 3.7067222947338387 MSE test 4.651036652945851\n",
            "Epoch 92 / 400 loss: 335.4246163368225\n",
            "MSE train 3.668572904238067 MSE test 4.613379494986144\n",
            "Epoch 93 / 400 loss: 331.9364800453186\n",
            "MSE train 3.6312551684153864 MSE test 4.5765162938347945\n",
            "Epoch 94 / 400 loss: 328.52809286117554\n",
            "MSE train 3.594760527504098 MSE test 4.540431537110743\n",
            "Epoch 95 / 400 loss: 325.19338512420654\n",
            "MSE train 3.5590034932037553 MSE test 4.505086668627005\n",
            "Epoch 96 / 400 loss: 321.93200755119324\n",
            "MSE train 3.523970727264279 MSE test 4.470425308079704\n",
            "Epoch 97 / 400 loss: 318.735337972641\n",
            "MSE train 3.4894834345057326 MSE test 4.436400969588349\n",
            "Epoch 98 / 400 loss: 315.60219526290894\n",
            "MSE train 3.455401536523476 MSE test 4.402941930715398\n",
            "Epoch 99 / 400 loss: 312.51577067375183\n",
            "MSE train 3.4217298269914145 MSE test 4.370049024848964\n",
            "Epoch 100 / 400 loss: 309.4648025035858\n",
            "MSE train 3.3884186553108515 MSE test 4.337673368240392\n",
            "Epoch 101 / 400 loss: 306.4489715099335\n",
            "MSE train 3.3557066765035475 MSE test 4.305881044842232\n",
            "Epoch 102 / 400 loss: 303.46688652038574\n",
            "MSE train 3.323582389209696 MSE test 4.27470552880763\n",
            "Epoch 103 / 400 loss: 300.5396189689636\n",
            "MSE train 3.2919161305508964 MSE test 4.244103086861731\n",
            "Epoch 104 / 400 loss: 297.6638720035553\n",
            "MSE train 3.260808418529197 MSE test 4.214095926624349\n",
            "Epoch 105 / 400 loss: 294.8304661512375\n",
            "MSE train 3.2304637140577395 MSE test 4.18470990552592\n",
            "Epoch 106 / 400 loss: 292.0494601726532\n",
            "MSE train 3.2008336199035785 MSE test 4.155918543605152\n",
            "Epoch 107 / 400 loss: 289.33752155303955\n",
            "MSE train 3.171912036663973 MSE test 4.127744473375201\n",
            "Epoch 108 / 400 loss: 286.689591050148\n",
            "MSE train 3.1435779049673003 MSE test 4.100116950239069\n",
            "Epoch 109 / 400 loss: 284.1035211086273\n",
            "MSE train 3.1157863198354843 MSE test 4.073021151108657\n",
            "Epoch 110 / 400 loss: 281.56949174404144\n",
            "MSE train 3.0885909404715957 MSE test 4.046448121936585\n",
            "Epoch 111 / 400 loss: 279.08397924900055\n",
            "MSE train 3.0620280296153513 MSE test 4.020401748506539\n",
            "Epoch 112 / 400 loss: 276.6524714231491\n",
            "MSE train 3.036077889976759 MSE test 3.994868266521284\n",
            "Epoch 113 / 400 loss: 274.27702033519745\n",
            "MSE train 3.010699398978342 MSE test 3.969835678533766\n",
            "Epoch 114 / 400 loss: 271.9558537006378\n",
            "MSE train 2.9857971344160994 MSE test 3.9452608798366455\n",
            "Epoch 115 / 400 loss: 269.684876203537\n",
            "MSE train 2.96140155119758 MSE test 3.9211177218438147\n",
            "Epoch 116 / 400 loss: 267.4561951160431\n",
            "MSE train 2.937449620411162 MSE test 3.8973886175437054\n",
            "Epoch 117 / 400 loss: 265.27262830734253\n",
            "MSE train 2.913900052747495 MSE test 3.8740491538629414\n",
            "Epoch 118 / 400 loss: 263.12801587581635\n",
            "MSE train 2.8907912072990474 MSE test 3.851132754390565\n",
            "Epoch 119 / 400 loss: 261.0197765827179\n",
            "MSE train 2.8680714711945354 MSE test 3.8286060934415436\n",
            "Epoch 120 / 400 loss: 258.950156211853\n",
            "MSE train 2.8457300833764236 MSE test 3.8064671600111932\n",
            "Epoch 121 / 400 loss: 256.9149261713028\n",
            "MSE train 2.823803665418206 MSE test 3.784705147256526\n",
            "Epoch 122 / 400 loss: 254.91453766822815\n",
            "MSE train 2.8020818402915144 MSE test 3.763266383903646\n",
            "Epoch 123 / 400 loss: 252.94945919513702\n",
            "MSE train 2.7806505442025666 MSE test 3.7421409684235396\n",
            "Epoch 124 / 400 loss: 251.00246036052704\n",
            "MSE train 2.7595589286450366 MSE test 3.7213604125983872\n",
            "Epoch 125 / 400 loss: 249.08242058753967\n",
            "MSE train 2.7387856279175584 MSE test 3.7009025520420655\n",
            "Epoch 126 / 400 loss: 247.19263994693756\n",
            "MSE train 2.718335764411607 MSE test 3.680763473189253\n",
            "Epoch 127 / 400 loss: 245.3315417766571\n",
            "MSE train 2.6982104019607625 MSE test 3.660940745678952\n",
            "Epoch 128 / 400 loss: 243.4994090795517\n",
            "MSE train 2.6783920488052013 MSE test 3.641427018734361\n",
            "Epoch 129 / 400 loss: 241.69630920886993\n",
            "MSE train 2.658843572916322 MSE test 3.62220935757554\n",
            "Epoch 130 / 400 loss: 239.92059767246246\n",
            "MSE train 2.6395752652275823 MSE test 3.6032776762346876\n",
            "Epoch 131 / 400 loss: 238.16915571689606\n",
            "MSE train 2.620617290936246 MSE test 3.5846276747320722\n",
            "Epoch 132 / 400 loss: 236.4431198835373\n",
            "MSE train 2.6019847001806 MSE test 3.5662564000179695\n",
            "Epoch 133 / 400 loss: 234.7450751066208\n",
            "MSE train 2.58372107717282 MSE test 3.548169952381797\n",
            "Epoch 134 / 400 loss: 233.07668197155\n",
            "MSE train 2.565771221060319 MSE test 3.5303585471463035\n",
            "Epoch 135 / 400 loss: 231.4410024881363\n",
            "MSE train 2.5480558071130597 MSE test 3.512793618999954\n",
            "Epoch 136 / 400 loss: 229.83256614208221\n",
            "MSE train 2.530576610017966 MSE test 3.4955029512366296\n",
            "Epoch 137 / 400 loss: 228.2455039024353\n",
            "MSE train 2.513333987905629 MSE test 3.478447491446763\n",
            "Epoch 138 / 400 loss: 226.67885887622833\n",
            "MSE train 2.496424252551246 MSE test 3.46167067763621\n",
            "Epoch 139 / 400 loss: 225.13545155525208\n",
            "MSE train 2.4798453468663144 MSE test 3.445180549193825\n",
            "Epoch 140 / 400 loss: 223.62110543251038\n",
            "MSE train 2.463596150539561 MSE test 3.42897562032256\n",
            "Epoch 141 / 400 loss: 222.13668954372406\n",
            "MSE train 2.447616538928942 MSE test 3.413027243064399\n",
            "Epoch 142 / 400 loss: 220.6815061569214\n",
            "MSE train 2.431876927293335 MSE test 3.397316930483705\n",
            "Epoch 143 / 400 loss: 219.25000965595245\n",
            "MSE train 2.416355675231817 MSE test 3.381835406905762\n",
            "Epoch 144 / 400 loss: 217.83988738059998\n",
            "MSE train 2.401029567208193 MSE test 3.366570381785245\n",
            "Epoch 145 / 400 loss: 216.44884943962097\n",
            "MSE train 2.3859684567900814 MSE test 3.351519645821369\n",
            "Epoch 146 / 400 loss: 215.0756903886795\n",
            "MSE train 2.3711311655008336 MSE test 3.3366816204703174\n",
            "Epoch 147 / 400 loss: 213.72617268562317\n",
            "MSE train 2.3565323918952394 MSE test 3.3220481700118127\n",
            "Epoch 148 / 400 loss: 212.39630734920502\n",
            "MSE train 2.342182395386638 MSE test 3.3076184945871594\n",
            "Epoch 149 / 400 loss: 211.08892607688904\n",
            "MSE train 2.3280452217325083 MSE test 3.293381747536466\n",
            "Epoch 150 / 400 loss: 209.80288767814636\n",
            "MSE train 2.314163316674514 MSE test 3.2793457392132255\n",
            "Epoch 151 / 400 loss: 208.53608524799347\n",
            "MSE train 2.3004775389266094 MSE test 3.265504897649483\n",
            "Epoch 152 / 400 loss: 207.29194581508636\n",
            "MSE train 2.286992202328843 MSE test 3.251853468367955\n",
            "Epoch 153 / 400 loss: 206.06496047973633\n",
            "MSE train 2.2736542234056456 MSE test 3.238382200175592\n",
            "Epoch 154 / 400 loss: 204.85564661026\n",
            "MSE train 2.260609240837469 MSE test 3.2250967845988443\n",
            "Epoch 155 / 400 loss: 203.65981888771057\n",
            "MSE train 2.247784675058509 MSE test 3.2120053071862413\n",
            "Epoch 156 / 400 loss: 202.49045324325562\n",
            "MSE train 2.235118814161716 MSE test 3.1990575887114696\n",
            "Epoch 157 / 400 loss: 201.3398517370224\n",
            "MSE train 2.2227531732462156 MSE test 3.1863342381059\n",
            "Epoch 158 / 400 loss: 200.20422458648682\n",
            "MSE train 2.2105954714218865 MSE test 3.173798669285942\n",
            "Epoch 159 / 400 loss: 199.09529042243958\n",
            "MSE train 2.19857766752591 MSE test 3.1614203007022748\n",
            "Epoch 160 / 400 loss: 198.0041708946228\n",
            "MSE train 2.186666576914636 MSE test 3.1491918060207413\n",
            "Epoch 161 / 400 loss: 196.92534065246582\n",
            "MSE train 2.1748738815721445 MSE test 3.13710804437111\n",
            "Epoch 162 / 400 loss: 195.85584461688995\n",
            "MSE train 2.1632447530103263 MSE test 3.12519613178038\n",
            "Epoch 163 / 400 loss: 194.79777085781097\n",
            "MSE train 2.1517380635384216 MSE test 3.1134360148770917\n",
            "Epoch 164 / 400 loss: 193.7539051771164\n",
            "MSE train 2.140375882459728 MSE test 3.1018280645253444\n",
            "Epoch 165 / 400 loss: 192.72095346450806\n",
            "MSE train 2.1291285226108103 MSE test 3.090369316729684\n",
            "Epoch 166 / 400 loss: 191.70084154605865\n",
            "MSE train 2.118004520783062 MSE test 3.0790582384060747\n",
            "Epoch 167 / 400 loss: 190.69100952148438\n",
            "MSE train 2.106973698167142 MSE test 3.0678843064842716\n",
            "Epoch 168 / 400 loss: 189.6921191215515\n",
            "MSE train 2.0960664046291 MSE test 3.0568468824397974\n",
            "Epoch 169 / 400 loss: 188.7016271352768\n",
            "MSE train 2.085369396431456 MSE test 3.0459634603838692\n",
            "Epoch 170 / 400 loss: 187.7225193977356\n",
            "MSE train 2.0749016138052396 MSE test 3.035251456056222\n",
            "Epoch 171 / 400 loss: 186.76264584064484\n",
            "MSE train 2.0645842797568843 MSE test 3.024683936734602\n",
            "Epoch 172 / 400 loss: 185.8230528831482\n",
            "MSE train 2.0543740289426737 MSE test 3.0142414626618463\n",
            "Epoch 173 / 400 loss: 184.89676713943481\n",
            "MSE train 2.0443192617360775 MSE test 3.003940464675906\n",
            "Epoch 174 / 400 loss: 183.98027658462524\n",
            "MSE train 2.0343768221978653 MSE test 2.9937704311407503\n",
            "Epoch 175 / 400 loss: 183.07764077186584\n",
            "MSE train 2.024622074108836 MSE test 2.9837414186370017\n",
            "Epoch 176 / 400 loss: 182.18520367145538\n",
            "MSE train 2.015013611968139 MSE test 2.9738542366953014\n",
            "Epoch 177 / 400 loss: 181.30974459648132\n",
            "MSE train 2.0055441133920477 MSE test 2.964096648872402\n",
            "Epoch 178 / 400 loss: 180.4472268819809\n",
            "MSE train 1.9962092163313934 MSE test 2.954467064355442\n",
            "Epoch 179 / 400 loss: 179.5972399711609\n",
            "MSE train 1.98696154528445 MSE test 2.9449577490831236\n",
            "Epoch 180 / 400 loss: 178.75922191143036\n",
            "MSE train 1.977788980880615 MSE test 2.9355613154596103\n",
            "Epoch 181 / 400 loss: 177.92893493175507\n",
            "MSE train 1.9687638538462502 MSE test 2.9262958450144225\n",
            "Epoch 182 / 400 loss: 177.10580718517303\n",
            "MSE train 1.9598909242234337 MSE test 2.9171664183113837\n",
            "Epoch 183 / 400 loss: 176.29613602161407\n",
            "MSE train 1.9511391474191047 MSE test 2.908153227625677\n",
            "Epoch 184 / 400 loss: 175.4999737739563\n",
            "MSE train 1.9425122230250418 MSE test 2.8992498474209216\n",
            "Epoch 185 / 400 loss: 174.71460378170013\n",
            "MSE train 1.934040007209648 MSE test 2.8904568914325948\n",
            "Epoch 186 / 400 loss: 173.9405130147934\n",
            "MSE train 1.9257101711604674 MSE test 2.8817735111556995\n",
            "Epoch 187 / 400 loss: 173.18026685714722\n",
            "MSE train 1.9174765237830735 MSE test 2.8731932690436746\n",
            "Epoch 188 / 400 loss: 172.4326068162918\n",
            "MSE train 1.9093207382154294 MSE test 2.8647076025796445\n",
            "Epoch 189 / 400 loss: 171.6934142112732\n",
            "MSE train 1.9012626193418205 MSE test 2.8563185356932874\n",
            "Epoch 190 / 400 loss: 170.9612889289856\n",
            "MSE train 1.8932864874796285 MSE test 2.848019385473604\n",
            "Epoch 191 / 400 loss: 170.23784339427948\n",
            "MSE train 1.8853867755898976 MSE test 2.8398066273806815\n",
            "Epoch 192 / 400 loss: 169.52181458473206\n",
            "MSE train 1.8775720066393182 MSE test 2.8316772975695668\n",
            "Epoch 193 / 400 loss: 168.8126142024994\n",
            "MSE train 1.8698566212830625 MSE test 2.8236342748071883\n",
            "Epoch 194 / 400 loss: 168.1111923456192\n",
            "MSE train 1.8622024382377749 MSE test 2.815666535896716\n",
            "Epoch 195 / 400 loss: 167.41857063770294\n",
            "MSE train 1.8547428255262661 MSE test 2.807805843322889\n",
            "Epoch 196 / 400 loss: 166.73177778720856\n",
            "MSE train 1.8474193993545909 MSE test 2.8000400066413658\n",
            "Epoch 197 / 400 loss: 166.062326669693\n",
            "MSE train 1.84019161120179 MSE test 2.792364923279487\n",
            "Epoch 198 / 400 loss: 165.4049825668335\n",
            "MSE train 1.8330305404000864 MSE test 2.7847724497353985\n",
            "Epoch 199 / 400 loss: 164.7560453414917\n",
            "MSE train 1.825930123228088 MSE test 2.777260776558137\n",
            "Epoch 200 / 400 loss: 164.1129847764969\n",
            "MSE train 1.8188730364625323 MSE test 2.7698299497806493\n",
            "Epoch 201 / 400 loss: 163.4753332734108\n",
            "MSE train 1.8118433786551063 MSE test 2.7624765480012843\n",
            "Epoch 202 / 400 loss: 162.8415687084198\n",
            "MSE train 1.8048798800768886 MSE test 2.7552026381228454\n",
            "Epoch 203 / 400 loss: 162.21032691001892\n",
            "MSE train 1.7980230922695994 MSE test 2.7480116904742524\n",
            "Epoch 204 / 400 loss: 161.5851663351059\n",
            "MSE train 1.7912352776069673 MSE test 2.7408954164855093\n",
            "Epoch 205 / 400 loss: 160.96958768367767\n",
            "MSE train 1.7844847317402102 MSE test 2.733845199117713\n",
            "Epoch 206 / 400 loss: 160.36010873317719\n",
            "MSE train 1.7777668879716433 MSE test 2.7268591576425822\n",
            "Epoch 207 / 400 loss: 159.75390022993088\n",
            "MSE train 1.771114946399798 MSE test 2.7199450701217676\n",
            "Epoch 208 / 400 loss: 159.15062403678894\n",
            "MSE train 1.7645172374782758 MSE test 2.713103315991632\n",
            "Epoch 209 / 400 loss: 158.55329775810242\n",
            "MSE train 1.757932577829648 MSE test 2.7063261103028178\n",
            "Epoch 210 / 400 loss: 157.96073561906815\n",
            "MSE train 1.7514019690518616 MSE test 2.6996136284007988\n",
            "Epoch 211 / 400 loss: 157.3693116903305\n",
            "MSE train 1.744976236957162 MSE test 2.692974929413764\n",
            "Epoch 212 / 400 loss: 156.78297632932663\n",
            "MSE train 1.73863091845949 MSE test 2.6864077128800816\n",
            "Epoch 213 / 400 loss: 156.2060500383377\n",
            "MSE train 1.7323278454973277 MSE test 2.6799020069623474\n",
            "Epoch 214 / 400 loss: 155.63626807928085\n",
            "MSE train 1.7260636224366863 MSE test 2.673452470562604\n",
            "Epoch 215 / 400 loss: 155.07027208805084\n",
            "MSE train 1.7198347790788486 MSE test 2.667062358960295\n",
            "Epoch 216 / 400 loss: 154.50780284404755\n",
            "MSE train 1.7136708282765156 MSE test 2.660736900596964\n",
            "Epoch 217 / 400 loss: 153.94847530126572\n",
            "MSE train 1.7076546837863633 MSE test 2.6544952816393677\n",
            "Epoch 218 / 400 loss: 153.39538782835007\n",
            "MSE train 1.7017397720641023 MSE test 2.6483351786846154\n",
            "Epoch 219 / 400 loss: 152.85557329654694\n",
            "MSE train 1.6958920713895795 MSE test 2.6422447309206993\n",
            "Epoch 220 / 400 loss: 152.32464230060577\n",
            "MSE train 1.6901152761864342 MSE test 2.6362244125395597\n",
            "Epoch 221 / 400 loss: 151.79975020885468\n",
            "MSE train 1.6843962018759615 MSE test 2.630273648561116\n",
            "Epoch 222 / 400 loss: 151.28122359514236\n",
            "MSE train 1.6787193834105574 MSE test 2.6243870290727416\n",
            "Epoch 223 / 400 loss: 150.76783055067062\n",
            "MSE train 1.6730827684134373 MSE test 2.6185608537026157\n",
            "Epoch 224 / 400 loss: 150.2582088112831\n",
            "MSE train 1.6675023220564 MSE test 2.6127941796509115\n",
            "Epoch 225 / 400 loss: 149.75224041938782\n",
            "MSE train 1.6619922645993526 MSE test 2.6070878319229016\n",
            "Epoch 226 / 400 loss: 149.25138241052628\n",
            "MSE train 1.6565294289779555 MSE test 2.6014391018585856\n",
            "Epoch 227 / 400 loss: 148.75683081150055\n",
            "MSE train 1.6511532101453084 MSE test 2.5958444513677734\n",
            "Epoch 228 / 400 loss: 148.26652014255524\n",
            "MSE train 1.6458442429667621 MSE test 2.590301374429912\n",
            "Epoch 229 / 400 loss: 147.7841054201126\n",
            "MSE train 1.6405642240794616 MSE test 2.584798233716505\n",
            "Epoch 230 / 400 loss: 147.30759155750275\n",
            "MSE train 1.6353457260108974 MSE test 2.579342770568522\n",
            "Epoch 231 / 400 loss: 146.83363980054855\n",
            "MSE train 1.6301894552346465 MSE test 2.57395022202715\n",
            "Epoch 232 / 400 loss: 146.36523431539536\n",
            "MSE train 1.6250684686487786 MSE test 2.5686019178032624\n",
            "Epoch 233 / 400 loss: 145.90236848592758\n",
            "MSE train 1.61998205878681 MSE test 2.5632939328892284\n",
            "Epoch 234 / 400 loss: 145.4426228404045\n",
            "MSE train 1.6149274632577895 MSE test 2.558022876505187\n",
            "Epoch 235 / 400 loss: 144.9859328866005\n",
            "MSE train 1.6098745570867938 MSE test 2.552783677897042\n",
            "Epoch 236 / 400 loss: 144.53196573257446\n",
            "MSE train 1.6048063748990766 MSE test 2.547572531543127\n",
            "Epoch 237 / 400 loss: 144.07801234722137\n",
            "MSE train 1.5997192550511103 MSE test 2.542388164972608\n",
            "Epoch 238 / 400 loss: 143.62258756160736\n",
            "MSE train 1.5946503140949064 MSE test 2.537235287540046\n",
            "Epoch 239 / 400 loss: 143.16545873880386\n",
            "MSE train 1.5896699829364818 MSE test 2.532131464556082\n",
            "Epoch 240 / 400 loss: 142.71028119325638\n",
            "MSE train 1.5847596409003535 MSE test 2.5270760983270946\n",
            "Epoch 241 / 400 loss: 142.262985765934\n",
            "MSE train 1.5799094820053592 MSE test 2.522070186707674\n",
            "Epoch 242 / 400 loss: 141.82194024324417\n",
            "MSE train 1.5750902718484585 MSE test 2.5171092982882093\n",
            "Epoch 243 / 400 loss: 141.38618326187134\n",
            "MSE train 1.570290924561063 MSE test 2.512191573553662\n",
            "Epoch 244 / 400 loss: 140.95311772823334\n",
            "MSE train 1.565557939481398 MSE test 2.507325364751438\n",
            "Epoch 245 / 400 loss: 140.52192080020905\n",
            "MSE train 1.560903153169743 MSE test 2.5025227461204733\n",
            "Epoch 246 / 400 loss: 140.09683644771576\n",
            "MSE train 1.5562994706503013 MSE test 2.4977744066869696\n",
            "Epoch 247 / 400 loss: 139.6787205338478\n",
            "MSE train 1.5517422133900045 MSE test 2.49307508912937\n",
            "Epoch 248 / 400 loss: 139.2651948928833\n",
            "MSE train 1.5472446833154754 MSE test 2.488421048118767\n",
            "Epoch 249 / 400 loss: 138.85586071014404\n",
            "MSE train 1.5428112633354887 MSE test 2.4838093642061003\n",
            "Epoch 250 / 400 loss: 138.4519454240799\n",
            "MSE train 1.5384207987004883 MSE test 2.4792371061521385\n",
            "Epoch 251 / 400 loss: 138.05376559495926\n",
            "MSE train 1.5340584373567139 MSE test 2.474702165881318\n",
            "Epoch 252 / 400 loss: 137.65939408540726\n",
            "MSE train 1.5297218321933985 MSE test 2.4702031650078786\n",
            "Epoch 253 / 400 loss: 137.26752191781998\n",
            "MSE train 1.5254168747808299 MSE test 2.46573992660558\n",
            "Epoch 254 / 400 loss: 136.87796235084534\n",
            "MSE train 1.5211446719672705 MSE test 2.461313715415151\n",
            "Epoch 255 / 400 loss: 136.49127155542374\n",
            "MSE train 1.516885020168435 MSE test 2.456925927904713\n",
            "Epoch 256 / 400 loss: 136.10757267475128\n",
            "MSE train 1.512685071663208 MSE test 2.452578702950979\n",
            "Epoch 257 / 400 loss: 135.72499233484268\n",
            "MSE train 1.5085822490377816 MSE test 2.448286350820708\n",
            "Epoch 258 / 400 loss: 135.34786128997803\n",
            "MSE train 1.5045335354348623 MSE test 2.4440390583069744\n",
            "Epoch 259 / 400 loss: 134.97937816381454\n",
            "MSE train 1.5005375367693896 MSE test 2.4398329450830727\n",
            "Epoch 260 / 400 loss: 134.61572796106339\n",
            "MSE train 1.496596177548435 MSE test 2.435666137206717\n",
            "Epoch 261 / 400 loss: 134.2568183541298\n",
            "MSE train 1.4926872677106338 MSE test 2.431535477612564\n",
            "Epoch 262 / 400 loss: 133.90275257825851\n",
            "MSE train 1.4887876220946301 MSE test 2.4274375739433864\n",
            "Epoch 263 / 400 loss: 133.5515038371086\n",
            "MSE train 1.48486240136897 MSE test 2.4233685923462724\n",
            "Epoch 264 / 400 loss: 133.2009584903717\n",
            "MSE train 1.4809491374404138 MSE test 2.419326234474959\n",
            "Epoch 265 / 400 loss: 132.84804099798203\n",
            "MSE train 1.4770887813651705 MSE test 2.4153161423379954\n",
            "Epoch 266 / 400 loss: 132.49669045209885\n",
            "MSE train 1.473255405720776 MSE test 2.411334975674861\n",
            "Epoch 267 / 400 loss: 132.14980244636536\n",
            "MSE train 1.4694439831471158 MSE test 2.407382432389696\n",
            "Epoch 268 / 400 loss: 131.8053162097931\n",
            "MSE train 1.4656530931996337 MSE test 2.4034572306161848\n",
            "Epoch 269 / 400 loss: 131.4628021121025\n",
            "MSE train 1.4618999745124448 MSE test 2.3995596172489995\n",
            "Epoch 270 / 400 loss: 131.12217718362808\n",
            "MSE train 1.458199376401936 MSE test 2.39569123883357\n",
            "Epoch 271 / 400 loss: 130.78499776124954\n",
            "MSE train 1.4545463553029 MSE test 2.39185042736958\n",
            "Epoch 272 / 400 loss: 130.45251733064651\n",
            "MSE train 1.450926626693221 MSE test 2.388033808554166\n",
            "Epoch 273 / 400 loss: 130.1242726445198\n",
            "MSE train 1.4473251160936351 MSE test 2.3842376952912483\n",
            "Epoch 274 / 400 loss: 129.79899150133133\n",
            "MSE train 1.4437415821959765 MSE test 2.38045926424717\n",
            "Epoch 275 / 400 loss: 129.47540694475174\n",
            "MSE train 1.4401742470104368 MSE test 2.3766968610105104\n",
            "Epoch 276 / 400 loss: 129.15346443653107\n",
            "MSE train 1.4366170440641832 MSE test 2.3729497781069\n",
            "Epoch 277 / 400 loss: 128.8329091668129\n",
            "MSE train 1.4330575589590147 MSE test 2.3692169407324406\n",
            "Epoch 278 / 400 loss: 128.5132071375847\n",
            "MSE train 1.4294771776959905 MSE test 2.3654964648919052\n",
            "Epoch 279 / 400 loss: 128.1932337284088\n",
            "MSE train 1.4258622860552832 MSE test 2.3617889952508606\n",
            "Epoch 280 / 400 loss: 127.87133514881134\n",
            "MSE train 1.4222382309942987 MSE test 2.3580982191236757\n",
            "Epoch 281 / 400 loss: 127.54635214805603\n",
            "MSE train 1.4186718111935421 MSE test 2.354444503930852\n",
            "Epoch 282 / 400 loss: 127.22082233428955\n",
            "MSE train 1.4151255576814246 MSE test 2.3508197771112984\n",
            "Epoch 283 / 400 loss: 126.90039652585983\n",
            "MSE train 1.41160222614412 MSE test 2.3472229239086975\n",
            "Epoch 284 / 400 loss: 126.58173131942749\n",
            "MSE train 1.4081262586843863 MSE test 2.343657364910245\n",
            "Epoch 285 / 400 loss: 126.2651184797287\n",
            "MSE train 1.4047046486887327 MSE test 2.3401265049391395\n",
            "Epoch 286 / 400 loss: 125.95278245210648\n",
            "MSE train 1.4013252617776166 MSE test 2.3366288451251997\n",
            "Epoch 287 / 400 loss: 125.64534932374954\n",
            "MSE train 1.3979857837342888 MSE test 2.3331631403509734\n",
            "Epoch 288 / 400 loss: 125.34172338247299\n",
            "MSE train 1.394694034375387 MSE test 2.3297310343529807\n",
            "Epoch 289 / 400 loss: 125.04169744253159\n",
            "MSE train 1.3914545804753098 MSE test 2.326334359741557\n",
            "Epoch 290 / 400 loss: 124.74596005678177\n",
            "MSE train 1.3882667444536825 MSE test 2.322972479738286\n",
            "Epoch 291 / 400 loss: 124.45492041110992\n",
            "MSE train 1.3851226213663297 MSE test 2.3196433195710635\n",
            "Epoch 292 / 400 loss: 124.16850954294205\n",
            "MSE train 1.382011468407699 MSE test 2.3163434901296265\n",
            "Epoch 293 / 400 loss: 123.88598471879959\n",
            "MSE train 1.3789263379519137 MSE test 2.313070052386713\n",
            "Epoch 294 / 400 loss: 123.60638999938965\n",
            "MSE train 1.3758617677205929 MSE test 2.309821078658806\n",
            "Epoch 295 / 400 loss: 123.32911628484726\n",
            "MSE train 1.3728103015416975 MSE test 2.3065944499190025\n",
            "Epoch 296 / 400 loss: 123.05367577075958\n",
            "MSE train 1.3697641933738787 MSE test 2.3033882263696555\n",
            "Epoch 297 / 400 loss: 122.77939248085022\n",
            "MSE train 1.3667196385401705 MSE test 2.3002007972490692\n",
            "Epoch 298 / 400 loss: 122.50556814670563\n",
            "MSE train 1.3636753204512768 MSE test 2.2970316592302717\n",
            "Epoch 299 / 400 loss: 122.23187053203583\n",
            "MSE train 1.360645864765112 MSE test 2.2938822139477706\n",
            "Epoch 300 / 400 loss: 121.95819681882858\n",
            "MSE train 1.357670800530201 MSE test 2.290758548637412\n",
            "Epoch 301 / 400 loss: 121.6859028339386\n",
            "MSE train 1.3547421191260212 MSE test 2.287661601567547\n",
            "Epoch 302 / 400 loss: 121.41853761672974\n",
            "MSE train 1.3518454447163968 MSE test 2.284589514391092\n",
            "Epoch 303 / 400 loss: 121.15535396337509\n",
            "MSE train 1.3489735028991618 MSE test 2.2815410353746697\n",
            "Epoch 304 / 400 loss: 120.89506179094315\n",
            "MSE train 1.3461466567604496 MSE test 2.278518541131468\n",
            "Epoch 305 / 400 loss: 120.63701385259628\n",
            "MSE train 1.3433779163156634 MSE test 2.2755303523380146\n",
            "Epoch 306 / 400 loss: 120.38306057453156\n",
            "MSE train 1.340633372188822 MSE test 2.2725721214641976\n",
            "Epoch 307 / 400 loss: 120.13432532548904\n",
            "MSE train 1.3379040765277685 MSE test 2.2696402587780637\n",
            "Epoch 308 / 400 loss: 119.88772678375244\n",
            "MSE train 1.3352081305396992 MSE test 2.2667372516745337\n",
            "Epoch 309 / 400 loss: 119.64250230789185\n",
            "MSE train 1.3325255806391123 MSE test 2.263861571910846\n",
            "Epoch 310 / 400 loss: 119.40026342868805\n",
            "MSE train 1.3298423662347534 MSE test 2.2610089443611123\n",
            "Epoch 311 / 400 loss: 119.15919309854507\n",
            "MSE train 1.3271966498443482 MSE test 2.258180371842749\n",
            "Epoch 312 / 400 loss: 118.91806507110596\n",
            "MSE train 1.3246012092401296 MSE test 2.255381263229399\n",
            "Epoch 313 / 400 loss: 118.68037790060043\n",
            "MSE train 1.322024681355451 MSE test 2.25260589933025\n",
            "Epoch 314 / 400 loss: 118.44719046354294\n",
            "MSE train 1.3194652684661554 MSE test 2.2498527443252527\n",
            "Epoch 315 / 400 loss: 118.21569496393204\n",
            "MSE train 1.31693003176073 MSE test 2.247120999128923\n",
            "Epoch 316 / 400 loss: 117.98574131727219\n",
            "MSE train 1.314419121532941 MSE test 2.244409648555065\n",
            "Epoch 317 / 400 loss: 117.75796967744827\n",
            "MSE train 1.3119209815840318 MSE test 2.2417169654637847\n",
            "Epoch 318 / 400 loss: 117.53237181901932\n",
            "MSE train 1.309426886474765 MSE test 2.2390409116546874\n",
            "Epoch 319 / 400 loss: 117.30789548158646\n",
            "MSE train 1.3069315539122972 MSE test 2.236379922204237\n",
            "Epoch 320 / 400 loss: 117.08375990390778\n",
            "MSE train 1.3044295056310795 MSE test 2.2337321570097872\n",
            "Epoch 321 / 400 loss: 116.85949164628983\n",
            "MSE train 1.3019151224402188 MSE test 2.231095129238697\n",
            "Epoch 322 / 400 loss: 116.63460922241211\n",
            "MSE train 1.299436124991694 MSE test 2.2284747002620864\n",
            "Epoch 323 / 400 loss: 116.4086521267891\n",
            "MSE train 1.2969882029299311 MSE test 2.225881466233445\n",
            "Epoch 324 / 400 loss: 116.18594920635223\n",
            "MSE train 1.2945520697039574 MSE test 2.2233049802557168\n",
            "Epoch 325 / 400 loss: 115.96598207950592\n",
            "MSE train 1.2921284278427025 MSE test 2.2207448001799492\n",
            "Epoch 326 / 400 loss: 115.74708169698715\n",
            "MSE train 1.2897187281070566 MSE test 2.218200407803585\n",
            "Epoch 327 / 400 loss: 115.5292951464653\n",
            "MSE train 1.2873267200243792 MSE test 2.215671541437581\n",
            "Epoch 328 / 400 loss: 115.31275820732117\n",
            "MSE train 1.2849526989530593 MSE test 2.2131579403700057\n",
            "Epoch 329 / 400 loss: 115.09782433509827\n",
            "MSE train 1.2825974157 MSE test 2.2106593718546828\n",
            "Epoch 330 / 400 loss: 114.8845329284668\n",
            "MSE train 1.2802831313931364 MSE test 2.2081769497954307\n",
            "Epoch 331 / 400 loss: 114.67295396327972\n",
            "MSE train 1.2780157628823752 MSE test 2.205712921193634\n",
            "Epoch 332 / 400 loss: 114.4650810956955\n",
            "MSE train 1.275776677248628 MSE test 2.203266165864382\n",
            "Epoch 333 / 400 loss: 114.26140278577805\n",
            "MSE train 1.2735465304357483 MSE test 2.2008337241153297\n",
            "Epoch 334 / 400 loss: 114.06023287773132\n",
            "MSE train 1.2713110083116208 MSE test 2.198413097365567\n",
            "Epoch 335 / 400 loss: 113.8598205447197\n",
            "MSE train 1.2690936499799481 MSE test 2.196003773136724\n",
            "Epoch 336 / 400 loss: 113.65890693664551\n",
            "MSE train 1.2668863410467506 MSE test 2.193605833228214\n",
            "Epoch 337 / 400 loss: 113.45962780714035\n",
            "MSE train 1.2646669189982787 MSE test 2.1912178526340766\n",
            "Epoch 338 / 400 loss: 113.26120096445084\n",
            "MSE train 1.2624485076174894 MSE test 2.188840288872726\n",
            "Epoch 339 / 400 loss: 113.06165528297424\n",
            "MSE train 1.2602467215289601 MSE test 2.186475272160066\n",
            "Epoch 340 / 400 loss: 112.86220055818558\n",
            "MSE train 1.2580674867551096 MSE test 2.1841241553336523\n",
            "Epoch 341 / 400 loss: 112.66426372528076\n",
            "MSE train 1.2559239102914377 MSE test 2.181789241893486\n",
            "Epoch 342 / 400 loss: 112.46839648485184\n",
            "MSE train 1.2537977426295759 MSE test 2.179469389301572\n",
            "Epoch 343 / 400 loss: 112.27574133872986\n",
            "MSE train 1.2516682001609871 MSE test 2.1771613492749218\n",
            "Epoch 344 / 400 loss: 112.08462369441986\n",
            "MSE train 1.2495492471934788 MSE test 2.174865602470461\n",
            "Epoch 345 / 400 loss: 111.89319467544556\n",
            "MSE train 1.2474523650458835 MSE test 2.1725866563855076\n",
            "Epoch 346 / 400 loss: 111.70276433229446\n",
            "MSE train 1.2453662935955532 MSE test 2.170321787809084\n",
            "Epoch 347 / 400 loss: 111.51430809497833\n",
            "MSE train 1.2432887011788576 MSE test 2.1680698188090664\n",
            "Epoch 348 / 400 loss: 111.3268159031868\n",
            "MSE train 1.2412178395589546 MSE test 2.1658303382224773\n",
            "Epoch 349 / 400 loss: 111.14007079601288\n",
            "MSE train 1.239150470304213 MSE test 2.1636023557902955\n",
            "Epoch 350 / 400 loss: 110.95390981435776\n",
            "MSE train 1.2370909372163825 MSE test 2.1613851095947916\n",
            "Epoch 351 / 400 loss: 110.76804810762405\n",
            "MSE train 1.2350480373122663 MSE test 2.1591789295313\n",
            "Epoch 352 / 400 loss: 110.58291041851044\n",
            "MSE train 1.233022192542124 MSE test 2.156984139885595\n",
            "Epoch 353 / 400 loss: 110.39932000637054\n",
            "MSE train 1.2310101810919596 MSE test 2.1548006227579735\n",
            "Epoch 354 / 400 loss: 110.21727514266968\n",
            "MSE train 1.2290123386829206 MSE test 2.152628405653634\n",
            "Epoch 355 / 400 loss: 110.03645533323288\n",
            "MSE train 1.2270273220036663 MSE test 2.1504674751403825\n",
            "Epoch 356 / 400 loss: 109.85689228773117\n",
            "MSE train 1.225053754999074 MSE test 2.1483179119364797\n",
            "Epoch 357 / 400 loss: 109.67847156524658\n",
            "MSE train 1.2230959172921054 MSE test 2.1461802666776384\n",
            "Epoch 358 / 400 loss: 109.50108510255814\n",
            "MSE train 1.2211567045145695 MSE test 2.14405549546332\n",
            "Epoch 359 / 400 loss: 109.32513785362244\n",
            "MSE train 1.2192335135509933 MSE test 2.1419437347082053\n",
            "Epoch 360 / 400 loss: 109.15087467432022\n",
            "MSE train 1.2173206328582948 MSE test 2.1398439593970724\n",
            "Epoch 361 / 400 loss: 108.97805535793304\n",
            "MSE train 1.2154110261996565 MSE test 2.1377545125717443\n",
            "Epoch 362 / 400 loss: 108.80615812540054\n",
            "MSE train 1.21349865082433 MSE test 2.135674293512404\n",
            "Epoch 363 / 400 loss: 108.63454508781433\n",
            "MSE train 1.2115821471657937 MSE test 2.133603257443617\n",
            "Epoch 364 / 400 loss: 108.46267193555832\n",
            "MSE train 1.2096656215193635 MSE test 2.131542566100086\n",
            "Epoch 365 / 400 loss: 108.2904247045517\n",
            "MSE train 1.2077554128267431 MSE test 2.129493564989464\n",
            "Epoch 366 / 400 loss: 108.11817365884781\n",
            "MSE train 1.2058599495031266 MSE test 2.127456593534723\n",
            "Epoch 367 / 400 loss: 107.9464892745018\n",
            "MSE train 1.203985486207405 MSE test 2.12543157560014\n",
            "Epoch 368 / 400 loss: 107.77613186836243\n",
            "MSE train 1.2021257247678785 MSE test 2.123417576014114\n",
            "Epoch 369 / 400 loss: 107.60765534639359\n",
            "MSE train 1.2002667586376319 MSE test 2.121412809436212\n",
            "Epoch 370 / 400 loss: 107.44048517942429\n",
            "MSE train 1.1983979866006265 MSE test 2.119415633728896\n",
            "Epoch 371 / 400 loss: 107.27336341142654\n",
            "MSE train 1.1965163183111887 MSE test 2.1174250324006403\n",
            "Epoch 372 / 400 loss: 107.10534596443176\n",
            "MSE train 1.194628774642647 MSE test 2.1154410739237743\n",
            "Epoch 373 / 400 loss: 106.93616056442261\n",
            "MSE train 1.192745850404381 MSE test 2.1134642623952313\n",
            "Epoch 374 / 400 loss: 106.76643514633179\n",
            "MSE train 1.1908709248661566 MSE test 2.1114944844686474\n",
            "Epoch 375 / 400 loss: 106.5971087217331\n",
            "MSE train 1.1890004190193988 MSE test 2.109530853467768\n",
            "Epoch 376 / 400 loss: 106.42849141359329\n",
            "MSE train 1.1871280365564651 MSE test 2.1075720620603944\n",
            "Epoch 377 / 400 loss: 106.26025599241257\n",
            "MSE train 1.1852497234858406 MSE test 2.1056172608508157\n",
            "Epoch 378 / 400 loss: 106.091836810112\n",
            "MSE train 1.1833669682304917 MSE test 2.103666667158119\n",
            "Epoch 379 / 400 loss: 105.92288225889206\n",
            "MSE train 1.1814831835506823 MSE test 2.101721085377426\n",
            "Epoch 380 / 400 loss: 105.75353157520294\n",
            "MSE train 1.1795973143941652 MSE test 2.0997811508454705\n",
            "Epoch 381 / 400 loss: 105.58408731222153\n",
            "MSE train 1.1777108140122754 MSE test 2.097847793114753\n",
            "Epoch 382 / 400 loss: 105.41445165872574\n",
            "MSE train 1.1758354414579184 MSE test 2.095924300920837\n",
            "Epoch 383 / 400 loss: 105.24477672576904\n",
            "MSE train 1.1739730922402503 MSE test 2.0940124493774728\n",
            "Epoch 384 / 400 loss: 105.07611036300659\n",
            "MSE train 1.1721246174327886 MSE test 2.0921116669385578\n",
            "Epoch 385 / 400 loss: 104.9086030125618\n",
            "MSE train 1.1702892631724668 MSE test 2.090222129231958\n",
            "Epoch 386 / 400 loss: 104.74234664440155\n",
            "MSE train 1.1684654832609962 MSE test 2.0883440452323385\n",
            "Epoch 387 / 400 loss: 104.57728314399719\n",
            "MSE train 1.1666525026237515 MSE test 2.086477646561974\n",
            "Epoch 388 / 400 loss: 104.41327381134033\n",
            "MSE train 1.1648484451594114 MSE test 2.084622568403062\n",
            "Epoch 389 / 400 loss: 104.25024378299713\n",
            "MSE train 1.1630568917038815 MSE test 2.0827784209211253\n",
            "Epoch 390 / 400 loss: 104.08802086114883\n",
            "MSE train 1.1612878050696966 MSE test 2.0809457075144406\n",
            "Epoch 391 / 400 loss: 103.92693537473679\n",
            "MSE train 1.1595422100408792 MSE test 2.07912511595974\n",
            "Epoch 392 / 400 loss: 103.76789581775665\n",
            "MSE train 1.1578160027513025 MSE test 2.077316929786549\n",
            "Epoch 393 / 400 loss: 103.61100232601166\n",
            "MSE train 1.1561013687782133 MSE test 2.075520916470256\n",
            "Epoch 394 / 400 loss: 103.4558697938919\n",
            "MSE train 1.1543909219949497 MSE test 2.073736005948771\n",
            "Epoch 395 / 400 loss: 103.30178898572922\n",
            "MSE train 1.152683209416931 MSE test 2.0719615232657325\n",
            "Epoch 396 / 400 loss: 103.14810508489609\n",
            "MSE train 1.1509954069983932 MSE test 2.070197470667513\n",
            "Epoch 397 / 400 loss: 102.99468219280243\n",
            "MSE train 1.1493529430519223 MSE test 2.0684467966506994\n",
            "Epoch 398 / 400 loss: 102.8430603146553\n",
            "MSE train 1.147722236280595 MSE test 2.066705960753749\n",
            "Epoch 399 / 400 loss: 102.69552284479141\n"
          ]
        }
      ],
      "source": [
        "# running the model for a 200 epochs taking 100 users in batches\n",
        "# total improvement is printed out after each epoch\n",
        "MSE_train=[]\n",
        "MSE_val=[]\n",
        "for epoch in range(hm_epochs):\n",
        "    epoch_loss = 0    # initializing error as 0\n",
        "\n",
        "    for i in range(int(tot_users/batch_size)):\n",
        "        epoch_x = X_train[ i*batch_size : (i+1)*batch_size ]\n",
        "        _, c = sess.run([optimizer, meansq],\\\n",
        "               feed_dict={input_layer: epoch_x, \\\n",
        "               output_true: epoch_x})\n",
        "        epoch_loss += c\n",
        "\n",
        "    output_train = sess.run(output_layer,\\\n",
        "               feed_dict={input_layer:X_train})\n",
        "    output_test = sess.run(output_layer,\\\n",
        "                   feed_dict={input_layer:X_test})\n",
        "    MSE_train.append(MSE(output_train, X_train))\n",
        "    MSE_val.append(MSE(output_test, X_test))\n",
        "\n",
        "    print('MSE train', MSE(output_train, X_train),'MSE test', MSE(output_test, X_test))\n",
        "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJcKdBk3nRU9",
        "outputId": "a4207161-012f-402b-d057-b92ead27621a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.066705960753749"
            ]
          },
          "execution_count": 70,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MSE_val[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHpKhCb-nUro",
        "outputId": "9132980d-e7e7-4027-855b-88921cb5e9a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 131,  186,  225,  254,  667,  693,  695,  759,  764,  961,  985,\n",
              "        1027, 1028, 1030, 1040, 1048, 1050, 1148, 1230, 1413, 1449, 1469,\n",
              "        1493, 1504, 1507, 1513, 1539, 1582, 1613, 1639, 1656, 1681, 1973,\n",
              "        1988, 1991, 2049, 2057, 2063, 2085, 2087, 2135, 2163, 2182, 2193,\n",
              "        2201, 2204, 2255, 2259, 2275, 2283, 2299, 2313, 2315, 2318, 2347,\n",
              "        2358, 2383, 2394, 2449, 2459, 2526, 2534, 2550, 2556, 2572, 2589,\n",
              "        2592, 2603, 2608, 2640, 2686, 2743, 2758, 2764, 2789, 2843, 2851,\n",
              "        2868, 2870, 2889, 2898, 2906, 2965, 2973, 2995, 2997, 3013, 3030,\n",
              "        3036, 3044, 3051, 3117, 3119, 3146, 3162, 3164, 3177, 3191, 3226,\n",
              "        3250, 3274, 3279, 3321, 3336, 3341, 3356, 3358, 3365, 3376, 3379,\n",
              "        3380, 3385, 3390, 3391, 3406, 3416, 3432, 3451, 3460, 3462, 3467,\n",
              "        3472, 3473, 3477, 3490, 3499, 3510, 3531, 3536, 3544, 3558, 3559,\n",
              "        3571, 3575, 3586, 3591, 3593, 3607, 3609, 3610, 3612, 3627, 3634,\n",
              "        3635, 3669, 3683, 3707, 3724, 3732, 3749, 3753, 3763, 3765, 3795,\n",
              "        3810, 3811, 3816, 3825, 3861, 3865, 3872, 3880, 3883, 3886, 3891,\n",
              "        3894, 3905, 3910, 3920, 3927, 3928, 3941, 3960, 3969, 3973, 3982,\n",
              "        4009]),)"
            ]
          },
          "execution_count": 73,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_n=20\n",
        "sample_user = X_test.iloc[0,:]\n",
        "np.nonzero(np.array(sample_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2nJWfK1V-YL"
      },
      "source": [
        "Let's see what movies we recommends to the first user in X_test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70O0RHMmkkgP",
        "outputId": "e07855bb-d778-493a-e3f8-5d1aefb505f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(318, 1.5484942),\n",
              " (356, 1.4646944),\n",
              " (296, 1.4559566),\n",
              " (593, 1.3677274),\n",
              " (2571, 1.2942533),\n",
              " (260, 1.2072569),\n",
              " (527, 1.0912037),\n",
              " (480, 1.0426964),\n",
              " (110, 1.0359629),\n",
              " (50, 0.99943984),\n",
              " (1210, 0.99004734),\n",
              " (1196, 0.9767819),\n",
              " (858, 0.9754989),\n",
              " (1198, 0.9428718),\n",
              " (2959, 0.9395235),\n",
              " (4993, 0.9374274),\n",
              " (589, 0.8988301),\n",
              " (457, 0.89507425),\n",
              " (2858, 0.8761992),\n",
              " (150, 0.8695363)]"
            ]
          },
          "execution_count": 426,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pick a user\n",
        "top_n=20\n",
        "sample_user = X_test.iloc[0,:]\n",
        "# get the predicted ratings\n",
        "sample_user_pred = sess.run(output_layer, feed_dict={input_layer:[sample_user]}).squeeze()\n",
        "pred_dict={}\n",
        "for i,j in zip(X_test.columns,sample_user_pred):\n",
        "   pred_dict[i]=j\n",
        "top_dict=[(key, pred_dict[key]) for key in sorted(pred_dict, key=pred_dict.get, reverse=True)[:top_n]]\n",
        "top_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cGiIGUBXgEG",
        "outputId": "b5a11c0f-2b68-4b30-9bfc-c9df6ac09289"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.3734739 , 1.2280059 , 1.0382456 , ..., 1.056731  , 0.9944702 ,\n",
              "       0.99814665], dtype=float32)"
            ]
          },
          "execution_count": 427,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.exp(sample_user_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H28wwfE_XMn2"
      },
      "source": [
        "recommendations for the user:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDKXIlopOwGd",
        "outputId": "5d25a68d-f74d-4c69-fcb3-6f36dcc79ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: Shawshank Redemption, The (1994), with rating of 4.704380989074707\n",
            "2: Forrest Gump (1994), with rating of 4.326220989227295\n",
            "3: Pulp Fiction (1994), with rating of 4.288584232330322\n",
            "4: Silence of the Lambs, The (1991), with rating of 3.926417350769043\n",
            "5: Matrix, The (1999), with rating of 3.648271083831787\n",
            "6: Star Wars: Episode IV - A New Hope (1977), with rating of 3.3442983627319336\n",
            "7: Schindler's List (1993), with rating of 2.977856397628784\n",
            "8: Jurassic Park (1993), with rating of 2.83685564994812\n",
            "9: Braveheart (1995), with rating of 2.8178184032440186\n",
            "10: Usual Suspects, The (1995), with rating of 2.71675968170166\n",
            "11: Star Wars: Episode VI - Return of the Jedi (1983), with rating of 2.691361904144287\n",
            "12: Star Wars: Episode V - The Empire Strikes Back (1980), with rating of 2.655895709991455\n",
            "13: Godfather, The (1972), with rating of 2.6524903774261475\n",
            "14: Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981), with rating of 2.5673437118530273\n",
            "15: Fight Club (1999), with rating of 2.5587620735168457\n",
            "16: Lord of the Rings: The Fellowship of the Ring, The (2001), with rating of 2.553403854370117\n",
            "17: Terminator 2: Judgment Day (1991), with rating of 2.4567275047302246\n",
            "18: Fugitive, The (1993), with rating of 2.4475173950195312\n",
            "19: American Beauty (1999), with rating of 2.4017536640167236\n",
            "20: Apollo 13 (1995), with rating of 2.3858041763305664\n"
          ]
        }
      ],
      "source": [
        "for i,(idx, dist) in enumerate(top_dict):\n",
        "            print('{0}: {1}, with rating ''of {2}'.format(i+1,reverse_hashmap[idx], np.exp(dist)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "h9BEzlBckayG",
        "outputId": "06940eb4-7693-46d4-9d61-483b8e5ccd14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mean Square Error of Autorec')"
            ]
          },
          "execution_count": 435,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(MSE_train,range(len(MSE_train)),label='Train')\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('mean square error')\n",
        "plt.plot(MSE_val,range(len(MSE_val)),label='Validation')\n",
        "plt.title('Mean Square Error of Autorec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eUfGI4hmA1b"
      },
      "source": [
        "#3)RNN:#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VngYUBhn8GJ",
        "outputId": "d7b07acc-dbb4-4f9a-ead4-dbc6908ab2f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37733"
            ]
          },
          "execution_count": 106,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags['item id'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcIIGEeaUHCp"
      },
      "outputs": [],
      "source": [
        "tags=tags.set_index(['user id','item id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-1mqYP3UYg-"
      },
      "outputs": [],
      "source": [
        "rating=rating.set_index(['user id','item id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaRY4YNWWK5k",
        "outputId": "3bc51e00-a785-497d-ca8f-1ad77c6f600a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36613, 1)"
            ]
          },
          "execution_count": 109,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odKNG7n_WOPz",
        "outputId": "535e345d-9758-46fe-e37e-364d8551d505"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1250207, 1)"
            ]
          },
          "execution_count": 110,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXk15Q27UqY_"
      },
      "outputs": [],
      "source": [
        "dataframe=tags.join(rating).dropna()\n",
        "#df.set_index('key').join(other.set_index('key'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iZTMVb7RBMS",
        "outputId": "bccde7d1-c6d5-4139-8956-afd475ea9405"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['tag', 'rating'], dtype='object')"
            ]
          },
          "execution_count": 112,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframe.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXYNUJ5jXrJY"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPAqHBb8Ywdi"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import math\n",
        "\n",
        "all_letters = string.ascii_letters+ string.digits + string.punctuation\n",
        "n_letters = len(all_letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SAQ2Cs3Zh0X"
      },
      "outputs": [],
      "source": [
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBQ_768sZ6jJ",
        "outputId": "0a2d93c6-6b8c-48c0-f8eb-ed1cbdfc2343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Slusarski\n"
          ]
        }
      ],
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXpmM7K_Y5Ky"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "category_lines={}\n",
        "for i in range(dataframe.shape[0]):\n",
        "  if dataframe['rating'].iloc[i] not in category_lines:\n",
        "       category_lines[dataframe['rating'].iloc[i]]=[dataframe['tag'].iloc[i]]\n",
        "  else:\n",
        "       category_lines[dataframe['rating'].iloc[i]].append(dataframe['tag'].iloc[i])\n",
        "for i in category_lines:\n",
        "   category_lines[i]=np.unique(category_lines[i]).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysji3YfqY8b8"
      },
      "outputs": [],
      "source": [
        "# Build the category_lines dictionary, a list of names per language\n",
        "train_data = {}\n",
        "validation_data = {}\n",
        "all_categories = []\n",
        "\n",
        "\n",
        "for category in category_lines:\n",
        "    all_categories.append(category)\n",
        "    lines = category_lines[category]\n",
        "    random.shuffle(lines)\n",
        "    train_data[category] = lines[0:int(math.floor(0.8*len(lines)))]\n",
        "    validation_data[category] = lines[int(math.floor(0.8*len(lines)))+1:]\n",
        "\n",
        "n_categories = len(all_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clqyf1SNgj2X",
        "outputId": "7d13e3e2-4499-4972-ceb0-1eaed6c7fbe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 45,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObejYLi8gtq6",
        "outputId": "28a667c6-f6e4-4af1-8e56-370d18994b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "def sentenceToTensor(sentence):\n",
        "    tensor = torch.zeros(len(sentence), 1, n_letters)\n",
        "    for li, letter in enumerate(sentence):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "#print(letterToTensor('J')[0])\n",
        "\n",
        "#print(lineToTensor('Jones'))\n",
        "\n",
        "print(sentenceToTensor('Joes Harr'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV473snAgvSU"
      },
      "outputs": [],
      "source": [
        "class RNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN_LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.LSTM=nn.LSTMCell(input_size,hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        hidden, cell = self.LSTM(input, (hidden,cell))\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output,hidden,cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 128\n",
        "rnn_lstm = RNN_LSTM(n_letters, n_hidden, n_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aoeCQzZg5iv",
        "outputId": "c1033d33-0345-4b06-ac96-df3ae589370d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-2.3446, -2.3187, -2.3390, -2.2734, -2.3322, -2.3016, -2.2266, -2.2985,\n",
            "         -2.3081, -2.2889]], grad_fn=<LogSoftmaxBackward>)\n"
          ]
        }
      ],
      "source": [
        "input = sentenceToTensor('Annetta Harry')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "cell=torch.zeros(1, n_hidden)\n",
        "output,hidden,cell= rnn_lstm(input[0], hidden,cell)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4zKp6Uug8UT"
      },
      "outputs": [],
      "source": [
        "def train_LSTM(category_tensor, line_tensor):\n",
        "    hidden = rnn_lstm.initHidden()\n",
        "    cell= rnn_lstm.initHidden()\n",
        "    # reset gradient\n",
        "    rnn_lstm.zero_grad()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output,hidden,cell = rnn_lstm(line_tensor[i], hidden, cell)\n",
        "    loss = criterion_lstm(output, category_tensor)\n",
        "\n",
        "    # compute gradient by backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer_lstm.step()\n",
        "\n",
        "    return output, loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UoA09hgySqV",
        "outputId": "21606b91-7b6a-447e-de93-3741c4b875ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1.0, 6)\n"
          ]
        }
      ],
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXsINekFg-hA",
        "outputId": "f2efef23-948e-46c4-868e-e06c901604b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "category = 0.5 / line = twistending\n",
            "category = 0.5 / line = centralevent weddingpreparation\n",
            "category = 5.0 / line = imaginaryworld characters story philosophical\n",
            "category = 3.5 / line = bradleycooper\n",
            "category = 5.0 / line = captainamerica\n",
            "category = 3.5 / line = existentialism\n",
            "category = 0.5 / line = dccomics\n",
            "category = 1.5 / line = remakeoflettherightonein \n",
            "category = 3.0 / line = jacknicholson\n",
            "category = 3.5 / line = alzheimer sdisease\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(train_data[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = sentenceToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eVDtE6NhD2l"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBPWztWxhH39",
        "outputId": "25180a9d-47ed-469c-d8af-b3e1a46b2665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iter \tTrain% \tTime \t\tTrain_loss_lstm \tExample\n",
            "5000 \t6% \t(1m 0s) \t2.2943 \t\thallucinatory / 3.0 ✗ (1.0)\n",
            "10000 \t12% \t(2m 7s) \t2.2908 \t\tspace / 4.5 ✗ (2.5)\n",
            "15000 \t18% \t(3m 14s) \t2.2729 \t\tpolice / 0.5 ✗ (2.5)\n",
            "20000 \t25% \t(4m 22s) \t2.2845 \t\tactionpacked / 3.0 ✓\n",
            "25000 \t31% \t(5m 29s) \t2.2578 \t\tsocjopat / 1.0 ✓\n",
            "30000 \t37% \t(6m 36s) \t2.2737 \t\tnotfaithfultothebook / 1.5 ✗ (3.5)\n",
            "35000 \t43% \t(7m 42s) \t2.2491 \t\tmanvs nature / 1.5 ✗ (4.5)\n",
            "40000 \t50% \t(8m 50s) \t2.2442 \t\tidentitypolitics / 1.5 ✗ (4.0)\n",
            "45000 \t56% \t(9m 58s) \t2.2158 \t\toscar besteffects visualeffects  / 0.5 ✓\n",
            "50000 \t62% \t(11m 7s) \t2.2217 \t\thijacking / 0.5 ✗ (5.0)\n",
            "55000 \t68% \t(12m 16s) \t2.1782 \t\tsidneylumet / 1.0 ✗ (4.5)\n",
            "60000 \t75% \t(13m 25s) \t2.1721 \t\ttaikawaititi / 2.5 ✗ (5.0)\n",
            "65000 \t81% \t(14m 33s) \t2.1557 \t\ttruestory / 1.5 ✗ (0.5)\n",
            "70000 \t87% \t(15m 41s) \t2.1194 \t\tsexualharassment / 2.0 ✗ (3.0)\n",
            "75000 \t93% \t(16m 49s) \t2.0646 \t\tendingkindaruinedit / 4.0 ✗ (2.0)\n",
            "80000 \t100% \t(17m 56s) \t2.1026 \t\tslackers / 0.5 ✗ (5.0)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "criterion_lstm = nn.NLLLoss()\n",
        "rnn_lstm = RNN_LSTM(n_letters, n_hidden, n_categories)\n",
        "optimizer_lstm = torch.optim.Adam(rnn_lstm.parameters())\n",
        "\n",
        "n_iters = 80000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "train_loss_lstm = 0\n",
        "all_train_losses_lstm = []\n",
        "all_validation_losses_lstm = []\n",
        "all_validation_losses2_lstm = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "# Just return an output given a line\n",
        "def evaluate_lstm(line_tensor):\n",
        "    hidden = rnn_lstm.initHidden()\n",
        "    cell= rnn_lstm.initHidden()\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output,hidden,cell = rnn_lstm(line_tensor[i], hidden,cell)\n",
        "    return output\n",
        "\n",
        "def eval_dataset_lstm(dataset):\n",
        "    loss = 0\n",
        "    n_instances = 0\n",
        "    confusion = torch.zeros(n_categories, n_categories)\n",
        "    for category in all_categories:\n",
        "        category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
        "        n_instances += len(dataset[category])\n",
        "        for line in dataset[category]:\n",
        "            line_tensor = Variable(sentenceToTensor(line))\n",
        "            output = evaluate_lstm(line_tensor)\n",
        "            loss += criterion_lstm(output, category_tensor)\n",
        "            guess, guess_i = categoryFromOutput(output)\n",
        "            category_i = all_categories.index(category)\n",
        "            confusion[category_i][guess_i] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(n_categories):\n",
        "        confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "    return loss.item() / n_instances, confusion\n",
        "\n",
        "print('\\nIter \\tTrain% \\tTime \\t\\tTrain_loss_lstm \\tExample')\n",
        "start = time.time()\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train_LSTM(category_tensor, line_tensor)\n",
        "    train_loss_lstm += loss\n",
        "\n",
        "    # Print iter number, train loss average, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "        print('%d \\t%d%% \\t(%s) \\t%.4f \\t\\t%s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), train_loss_lstm / plot_every, line, guess, correct))\n",
        "\n",
        "    # Add current train loss average to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_train_losses_lstm.append(train_loss_lstm / plot_every)\n",
        "        train_loss_lstm = 0\n",
        "\n",
        "    # Compute loss based on validation data\n",
        "    if iter % plot_every == 0:\n",
        "        average_validation_loss_lstm, _ = eval_dataset_lstm(validation_data)\n",
        "\n",
        "        # save model with best validation loss\n",
        "        if len(all_validation_losses_lstm) == 0 or average_validation_loss_lstm < min(all_validation_losses_lstm):\n",
        "            torch.save(rnn_lstm, 'char_rnn_lstm_classification_model.pt')\n",
        "        all_validation_losses_lstm.append(average_validation_loss_lstm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMcftCcXl3YA",
        "outputId": "61f9842a-b5df-4b07-abfc-cccdb058ca25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.0886, 0.1009, 0.0519, 0.1706, 0.1320, 0.0672, 0.1087, 0.1105, 0.0438,\n",
              "          0.1257]]), [2.5])"
            ]
          },
          "execution_count": 86,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict(input_line, n_predictions=1):\n",
        "    #print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = torch.exp(evaluate_lstm(sentenceToTensor(input_line)))\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            #print(all_categories[category_index])\n",
        "            predictions.append(all_categories[category_index])\n",
        "    return(output, predictions)\n",
        "predict('father')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8e4MZSuMjrB",
        "outputId": "4a3a3093-7636-4ede-e817-c9ff3873a00b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.0766, 0.0735, 0.1013, 0.0542, 0.1355, 0.0893, 0.3054, 0.0684, 0.0292,\n",
              "          0.0665]]), [1.0])"
            ]
          },
          "execution_count": 90,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11k3dz5sJFUY",
        "outputId": "13923cdc-8b12-484d-b906-c93ab9047fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.094855375500958\n"
          ]
        }
      ],
      "source": [
        "#train accuracy:\n",
        "MSE=0\n",
        "count=0\n",
        "for i in train_data:\n",
        "  for j in train_data[i]:\n",
        "        output,pred=predict(j,n_predictions=1)\n",
        "        MSE+=(float(pred[0])-float(i))**2\n",
        "       # print(MSE)\n",
        "        count+=1\n",
        "print(MSE/count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY2BFKGwmGP6",
        "outputId": "d30c3dad-0362-44ce-efb7-ad832cb20124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.4090432960893855\n"
          ]
        }
      ],
      "source": [
        "#validation accuracy:\n",
        "MSE=0\n",
        "count=0\n",
        "for i in validation_data:\n",
        "  for j in validation_data[i]:\n",
        "        output,pred=predict(j,n_predictions=1)\n",
        "        MSE+=(float(pred[0])-float(i))**2\n",
        "       # print(MSE)\n",
        "        count+=1\n",
        "print(MSE/count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZj-ZRaAYW_z"
      },
      "source": [
        "#Use word to vec method:#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFsts9DlLm3e"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMS8Pp7ZYgTJ"
      },
      "outputs": [],
      "source": [
        "max_features = 440 #This is highest frequency threshold for all indices.\n",
        "maxlen = 3  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tl35uBcYitr"
      },
      "outputs": [],
      "source": [
        "import re  # For preprocessing\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import spacy  # For preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGiBDqRGqqT6"
      },
      "outputs": [],
      "source": [
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
        "    s=s.strip()\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ueLJyfU3qxS",
        "outputId": "69f9c44a-d870-4413-cae5-234bfbf68d35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "user id  item id\n",
              "42       37733      disappointing\n",
              "         37733          overrated\n",
              "         37733             stupid\n",
              "332      236             Meg Ryan\n",
              "         3408       Julia Roberts\n",
              "                        ...      \n",
              "526      6953             Tragedy\n",
              "         6975          dark humor\n",
              "         6975          disturbing\n",
              "         6975             torture\n",
              "         8507        banned movie\n",
              "Name: tag, Length: 100, dtype: object"
            ]
          },
          "execution_count": 120,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframe.tag[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnJErubi7Cm7",
        "outputId": "7fdbc1af-35f4-4afa-c3fb-812da8f95134"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(33454, 2)"
            ]
          },
          "execution_count": 121,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframe.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jflh3Ad0Ykgb",
        "outputId": "4dc7995d-8767-46b7-b0a6-fc4863fc5165"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33454"
            ]
          },
          "execution_count": 122,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_combine=dataframe['tag']\n",
        "len(text_combine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpmy9-jYqe6L",
        "outputId": "bee879d6-51b0-4c76-d61d-5d2b36998687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "user id  item id\n",
              "42       37733      disappointing\n",
              "         37733          overrated\n",
              "         37733             stupid\n",
              "332      236             Meg Ryan\n",
              "         3408       Julia Roberts\n",
              "                        ...      \n",
              "282989   2571        martial arts\n",
              "         2571               ulhas\n",
              "         6365             awesome\n",
              "283117   4011       Jason Statham\n",
              "         4011        twist ending\n",
              "Name: tag, Length: 33454, dtype: object"
            ]
          },
          "execution_count": 123,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_combine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vToiqvwElV4y",
        "outputId": "edbb02eb-45d0-4b9f-e8c6-94a918a007fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "user id  item id\n",
              "42       37733      disappointing\n",
              "         37733          overrated\n",
              "         37733             stupid\n",
              "332      236             Meg Ryan\n",
              "         3408       Julia Roberts\n",
              "                        ...      \n",
              "282989   2571        martial arts\n",
              "         2571               ulhas\n",
              "         6365             awesome\n",
              "283117   4011       Jason Statham\n",
              "         4011        twist ending\n",
              "Name: tag, Length: 33454, dtype: object"
            ]
          },
          "execution_count": 124,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_combine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R60UPPqBlLFk"
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases, Phraser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz1Je7MllNZ3",
        "outputId": "5c8f2105-4a6a-480e-d4d5-348b7d4cf27b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ],
      "source": [
        "#As Phrases() takes a list of list of words as input:\n",
        "#Creates the relevant phrases from the list of sentences:\n",
        "sent = [row.split() for row in text_combine]\n",
        "phrases = Phrases(sent, progress_per=10000)\n",
        "#The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:\n",
        "sentences = phrases[sent]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfbaOvUdlObt",
        "outputId": "b553ebe6-f959-4bd1-b952-727501c5b811"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "7427"
            ]
          },
          "execution_count": 127,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruDnXgdXm_bS",
        "outputId": "8b49c3e5-0dd0-46ce-d737-627339ab1206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['comedy',\n",
              " 'sci-fi',\n",
              " 'dark',\n",
              " 'action',\n",
              " 'atmospheric',\n",
              " 'surreal',\n",
              " 'based_on',\n",
              " 'funny',\n",
              " 'classic',\n",
              " 'visually_appealing']"
            ]
          },
          "execution_count": 128,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmRt9-bLnCQs"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyj7o4T7nu7x",
        "outputId": "df359810-fd4d-44ab-83b2-7cffbcc9c342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 130,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2IyMD2gny9P"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(min_count=0,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=cores-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Lqpry5n23q",
        "outputId": "8663cd10-2352-432c-a116-5398a83524e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.03 mins\n"
          ]
        }
      ],
      "source": [
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPVyHIeEn5IV"
      },
      "outputs": [],
      "source": [
        "w2v_model.init_sims(replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osMuc9BWn6x2",
        "outputId": "218f72e5-46f9-4829-c5e1-ca7fa979f50f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('surreal', 0.2451900690793991),\n",
              " ('Abigail', 0.2125636339187622),\n",
              " ('Eddie', 0.2085777074098587),\n",
              " ('tense', 0.20810966193675995),\n",
              " ('this', 0.20528408885002136),\n",
              " ('Lilly', 0.2009146511554718),\n",
              " ('Wulff', 0.19531753659248352),\n",
              " ('hanging', 0.19269561767578125),\n",
              " ('best', 0.19218936562538147),\n",
              " ('Spacey', 0.1895468682050705)]"
            ]
          },
          "execution_count": 134,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#most similiar words to \"us\"\n",
        "w2v_model.wv.most_similar(positive=[\"male\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gW33HFAn9PP"
      },
      "outputs": [],
      "source": [
        "model_name = \"product_content_clean\"\n",
        "w2v_model.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNP9UZZxoAMP"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"product_content_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X5y25wroDmG",
        "outputId": "5fb8d850-cc93-4706-a51d-a4f4dd89a7bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-0.03292004,  0.0008825 ,  0.04877856, ..., -0.03010911,\n",
              "        -0.03559132,  0.07979657],\n",
              "       [-0.00292705,  0.08412421,  0.06638248, ..., -0.02763497,\n",
              "        -0.04794508, -0.09617441],\n",
              "       [ 0.03382205,  0.07162233, -0.00565149, ..., -0.08166121,\n",
              "         0.07653634, -0.07900032],\n",
              "       ...,\n",
              "       [ 0.00538273, -0.0544017 ,  0.02036488, ...,  0.05994604,\n",
              "        -0.03334845, -0.01385388],\n",
              "       [-0.01945043,  0.02121533, -0.04944266, ...,  0.02177351,\n",
              "         0.08666735, -0.0569833 ],\n",
              "       [-0.03831702, -0.09806774, -0.08820985, ...,  0.06559112,\n",
              "         0.08154435,  0.0141946 ]], dtype=float32)"
            ]
          },
          "execution_count": 137,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors = model.wv.syn0 #array essentially holds raw word-vectors.these vectors are a 'projection layer' that can convert a one-hot encoding of a word into a dense embedding-vector of the right dimensionality.\n",
        "word_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWIiHathoGkW",
        "outputId": "45d377c6-07f7-4b9b-c00e-1cef0d323c9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 138,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#word2index is dictionary of all words as key and value is frequency in corpus:\n",
        "word2index = {token: token_index for token_index, token in enumerate(model.wv.index2word)}\n",
        "\"US\" in word2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3ZSo9VloHAj",
        "outputId": "458bfc47-f491-4b72-d265-79a40f203b9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7427"
            ]
          },
          "execution_count": 139,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBRvhvjXoNKb",
        "outputId": "aa425e59-257d-4b80-d554-4c828b73a3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comedy', 'sci-fi', 'dark', 'action', 'atmospheric', 'surreal', 'based_on', 'funny', 'classic', 'visually_appealing']\n",
            "865\n"
          ]
        }
      ],
      "source": [
        "print(model.wv.index2word[:10])\n",
        "word=\"male\"\n",
        "print(model.wv.vocab.get(word).index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui26IlvKoOCZ",
        "outputId": "26ed4f82-4960-41dd-dea5-0cdb9d6503c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ],
      "source": [
        "sentences_index=[]\n",
        "for news in sentences:\n",
        "    lst=[]\n",
        "    for word in news:\n",
        "      if word in word2index:#if the word is in corpus\n",
        "        if word2index[word]+1<max_features-1:#exclude extremely rare words\n",
        "           lst.append(word2index[word]+1)\n",
        "    sentences_index.append(lst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQTmUXFOoUP9",
        "outputId": "422c035f-33c7-4c29-df69-602b0983d45f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33454"
            ]
          },
          "execution_count": 142,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL3QU_2noU0e",
        "outputId": "12a39136-5089-4ef6-d836-0656e3d14cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 2 2 ... 8 9 9]\n",
            "(33454,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "label_list=dataframe['rating'].tolist()\n",
        "le.fit(label_list)\n",
        "transfomed_label =le.transform(label_list)\n",
        "print(transfomed_label)\n",
        "print(transfomed_label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx2NhUWRpvPY",
        "outputId": "aa485514-bbbe-4fc0-965b-7c2929c8f5ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33454"
            ]
          },
          "execution_count": 144,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnqRNdEJ5mL_",
        "outputId": "5a4856bc-a7d3-4df6-8e07-40ad78a11497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17207\n",
            "17207\n",
            "11946\n",
            "11946\n",
            "4301\n",
            "4301\n"
          ]
        }
      ],
      "source": [
        "x_train_text=sentences_index[:21508]\n",
        "x_test_text=sentences_index[21508:]\n",
        "x_val_text=x_train_text[-4301:]\n",
        "x_train_text=x_train_text[:-4301]\n",
        "y_train_text=dataframe['rating'].tolist()[:21508]\n",
        "y_test_text=dataframe['rating'].tolist()[21508:]\n",
        "y_val_text=y_train_text[-4301:]\n",
        "y_train_text=y_train_text[:-4301]\n",
        "print(len(x_train_text))\n",
        "print(len(y_train_text))\n",
        "print(len(x_test_text))\n",
        "print(len(y_test_text))\n",
        "print(len(x_val_text))\n",
        "print(len(y_val_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7INC2iRs9auI"
      },
      "outputs": [],
      "source": [
        "# create a callback that will save the best model while training\n",
        "checkpoint = ModelCheckpoint('best_model_text.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjGyrbWn950S",
        "outputId": "a359f42c-e9ad-430f-ed0c-ec2b7bff8cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (17207, 3)\n",
            "x_test shape: (4301, 3)\n"
          ]
        }
      ],
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train_text = sequence.pad_sequences(x_train_text, maxlen=maxlen)\n",
        "x_val_text = sequence.pad_sequences(x_val_text, maxlen=maxlen)\n",
        "print('x_train shape:', x_train_text.shape)\n",
        "print('x_test shape:', x_val_text.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShvD9r_A-TtK",
        "outputId": "93492214-d63b-4f63-c1c8-31e46fac5e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n"
          ]
        }
      ],
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_test_text = sequence.pad_sequences(x_test_text, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IVgPs0e-d1p",
        "outputId": "190b43b1-261a-4ad3-ccbf-75c01c1aca38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v1.py:59: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "#######################\n",
        "#  Embedding layer[4] #\n",
        "#######################\n",
        "# - the model will take as input an integer matrix of size (batch_size, input_length).\n",
        "# - the largest integer (i.e. word index) in the input should be no larger than (max_features-1)\n",
        "#   (vocabulary size).\n",
        "# - now model.output_shape == (None, input_length, 128), where None is the batch dimension.\n",
        "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PMOaOYI-vLd",
        "outputId": "c8f17f71-a83d-4339-9ccf-92717e6ea0e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 3, 128)            56320     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 188,033\n",
            "Trainable params: 188,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWxj17PTAcRq"
      },
      "outputs": [],
      "source": [
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "              optimizer='adam',\n",
        "metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fp_Z-IAAvqq",
        "outputId": "4f81d265-4270-407d-da32-042d1156e4fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Train on 17207 samples, validate on 4301 samples\n",
            "Epoch 1/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 1.5292 - mean_squared_error: 1.5292"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59216, saving model to best_model_text.h5\n",
            "17207/17207 [==============================] - 7s 398us/sample - loss: 1.5278 - mean_squared_error: 1.5278 - val_loss: 0.5922 - val_mean_squared_error: 0.5922\n",
            "Epoch 2/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.9564 - mean_squared_error: 0.9564\n",
            "Epoch 00002: val_loss did not improve from 0.59216\n",
            "17207/17207 [==============================] - 4s 247us/sample - loss: 0.9577 - mean_squared_error: 0.9577 - val_loss: 0.5933 - val_mean_squared_error: 0.5933\n",
            "Epoch 3/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.9382 - mean_squared_error: 0.9382\n",
            "Epoch 00003: val_loss did not improve from 0.59216\n",
            "17207/17207 [==============================] - 4s 247us/sample - loss: 0.9384 - mean_squared_error: 0.9384 - val_loss: 0.5924 - val_mean_squared_error: 0.5924\n",
            "Epoch 4/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.9303 - mean_squared_error: 0.9303\n",
            "Epoch 00004: val_loss did not improve from 0.59216\n",
            "17207/17207 [==============================] - 4s 251us/sample - loss: 0.9312 - mean_squared_error: 0.9312 - val_loss: 0.6269 - val_mean_squared_error: 0.6269\n",
            "Epoch 5/50\n",
            "17056/17207 [============================>.] - ETA: 0s - loss: 0.9253 - mean_squared_error: 0.9253\n",
            "Epoch 00005: val_loss improved from 0.59216 to 0.59068, saving model to best_model_text.h5\n",
            "17207/17207 [==============================] - 5s 319us/sample - loss: 0.9254 - mean_squared_error: 0.9254 - val_loss: 0.5907 - val_mean_squared_error: 0.5907\n",
            "Epoch 6/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.9210 - mean_squared_error: 0.9210\n",
            "Epoch 00006: val_loss improved from 0.59068 to 0.58362, saving model to best_model_text.h5\n",
            "17207/17207 [==============================] - 4s 249us/sample - loss: 0.9210 - mean_squared_error: 0.9210 - val_loss: 0.5836 - val_mean_squared_error: 0.5836\n",
            "Epoch 7/50\n",
            "17120/17207 [============================>.] - ETA: 0s - loss: 0.9199 - mean_squared_error: 0.9199\n",
            "Epoch 00007: val_loss did not improve from 0.58362\n",
            "17207/17207 [==============================] - 4s 248us/sample - loss: 0.9193 - mean_squared_error: 0.9193 - val_loss: 0.5940 - val_mean_squared_error: 0.5940\n",
            "Epoch 8/50\n",
            "17120/17207 [============================>.] - ETA: 0s - loss: 0.9154 - mean_squared_error: 0.9154\n",
            "Epoch 00008: val_loss improved from 0.58362 to 0.57338, saving model to best_model_text.h5\n",
            "17207/17207 [==============================] - 5s 312us/sample - loss: 0.9159 - mean_squared_error: 0.9159 - val_loss: 0.5734 - val_mean_squared_error: 0.5734\n",
            "Epoch 9/50\n",
            "17056/17207 [============================>.] - ETA: 0s - loss: 0.9176 - mean_squared_error: 0.9176\n",
            "Epoch 00009: val_loss improved from 0.57338 to 0.56437, saving model to best_model_text.h5\n",
            "17207/17207 [==============================] - 4s 250us/sample - loss: 0.9162 - mean_squared_error: 0.9162 - val_loss: 0.5644 - val_mean_squared_error: 0.5644\n",
            "Epoch 10/50\n",
            "16992/17207 [============================>.] - ETA: 0s - loss: 0.9155 - mean_squared_error: 0.9155\n",
            "Epoch 00010: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 247us/sample - loss: 0.9167 - mean_squared_error: 0.9167 - val_loss: 0.5695 - val_mean_squared_error: 0.5695\n",
            "Epoch 11/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.9089 - mean_squared_error: 0.9089\n",
            "Epoch 00011: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.9099 - mean_squared_error: 0.9099 - val_loss: 0.5922 - val_mean_squared_error: 0.5922\n",
            "Epoch 12/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.9093 - mean_squared_error: 0.9093\n",
            "Epoch 00012: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 251us/sample - loss: 0.9089 - mean_squared_error: 0.9089 - val_loss: 0.6077 - val_mean_squared_error: 0.6077\n",
            "Epoch 13/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 0.9106 - mean_squared_error: 0.9106\n",
            "Epoch 00013: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.9113 - mean_squared_error: 0.9113 - val_loss: 0.5979 - val_mean_squared_error: 0.5979\n",
            "Epoch 14/50\n",
            "17207/17207 [==============================] - ETA: 0s - loss: 0.9098 - mean_squared_error: 0.9098\n",
            "Epoch 00014: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 248us/sample - loss: 0.9098 - mean_squared_error: 0.9098 - val_loss: 0.5816 - val_mean_squared_error: 0.5816\n",
            "Epoch 15/50\n",
            "17120/17207 [============================>.] - ETA: 0s - loss: 0.9089 - mean_squared_error: 0.9089\n",
            "Epoch 00015: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 250us/sample - loss: 0.9082 - mean_squared_error: 0.9082 - val_loss: 0.5780 - val_mean_squared_error: 0.5780\n",
            "Epoch 16/50\n",
            "17120/17207 [============================>.] - ETA: 0s - loss: 0.9053 - mean_squared_error: 0.9053\n",
            "Epoch 00016: val_loss did not improve from 0.56437\n",
            "17207/17207 [==============================] - 4s 248us/sample - loss: 0.9052 - mean_squared_error: 0.9052 - val_loss: 0.6124 - val_mean_squared_error: 0.6124\n",
            "Epoch 17/50\n",
            "17056/17207 [============================>.] - ETA: 0s - loss: 0.9079 - mean_squared_error: 0.9079\n",
            "Epoch 00017: val_loss improved from 0.56437 to 0.56230, saving model to best_model_text.h5\n",
            "17207/17207 [==============================] - 5s 313us/sample - loss: 0.9070 - mean_squared_error: 0.9070 - val_loss: 0.5623 - val_mean_squared_error: 0.5623\n",
            "Epoch 18/50\n",
            "17120/17207 [============================>.] - ETA: 0s - loss: 0.9043 - mean_squared_error: 0.9043\n",
            "Epoch 00018: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 253us/sample - loss: 0.9044 - mean_squared_error: 0.9044 - val_loss: 0.6032 - val_mean_squared_error: 0.6032\n",
            "Epoch 19/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 0.9038 - mean_squared_error: 0.9038\n",
            "Epoch 00019: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 250us/sample - loss: 0.9034 - mean_squared_error: 0.9034 - val_loss: 0.6387 - val_mean_squared_error: 0.6387\n",
            "Epoch 20/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.9056 - mean_squared_error: 0.9056\n",
            "Epoch 00020: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 256us/sample - loss: 0.9044 - mean_squared_error: 0.9044 - val_loss: 0.5798 - val_mean_squared_error: 0.5798\n",
            "Epoch 21/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.9013 - mean_squared_error: 0.9013\n",
            "Epoch 00021: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 252us/sample - loss: 0.9018 - mean_squared_error: 0.9018 - val_loss: 0.6354 - val_mean_squared_error: 0.6354\n",
            "Epoch 22/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.9018 - mean_squared_error: 0.9018\n",
            "Epoch 00022: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 252us/sample - loss: 0.9020 - mean_squared_error: 0.9020 - val_loss: 0.6083 - val_mean_squared_error: 0.6083\n",
            "Epoch 23/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 0.9021 - mean_squared_error: 0.9021\n",
            "Epoch 00023: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 250us/sample - loss: 0.9001 - mean_squared_error: 0.9001 - val_loss: 0.5689 - val_mean_squared_error: 0.5689\n",
            "Epoch 24/50\n",
            "17207/17207 [==============================] - ETA: 0s - loss: 0.9007 - mean_squared_error: 0.9007\n",
            "Epoch 00024: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.9007 - mean_squared_error: 0.9007 - val_loss: 0.5991 - val_mean_squared_error: 0.5991\n",
            "Epoch 25/50\n",
            "17120/17207 [============================>.] - ETA: 0s - loss: 0.8976 - mean_squared_error: 0.8976\n",
            "Epoch 00025: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8985 - mean_squared_error: 0.8985 - val_loss: 0.6123 - val_mean_squared_error: 0.6123\n",
            "Epoch 26/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8980 - mean_squared_error: 0.8980\n",
            "Epoch 00026: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.8982 - mean_squared_error: 0.8982 - val_loss: 0.5856 - val_mean_squared_error: 0.5856\n",
            "Epoch 27/50\n",
            "17207/17207 [==============================] - ETA: 0s - loss: 0.8974 - mean_squared_error: 0.8974\n",
            "Epoch 00027: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 260us/sample - loss: 0.8974 - mean_squared_error: 0.8974 - val_loss: 0.5827 - val_mean_squared_error: 0.5827\n",
            "Epoch 28/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 0.8968 - mean_squared_error: 0.8968\n",
            "Epoch 00028: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 258us/sample - loss: 0.8952 - mean_squared_error: 0.8952 - val_loss: 0.6066 - val_mean_squared_error: 0.6066\n",
            "Epoch 29/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.8942 - mean_squared_error: 0.8942\n",
            "Epoch 00029: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 256us/sample - loss: 0.8954 - mean_squared_error: 0.8954 - val_loss: 0.6091 - val_mean_squared_error: 0.6091\n",
            "Epoch 30/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8977 - mean_squared_error: 0.8977\n",
            "Epoch 00030: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 258us/sample - loss: 0.8971 - mean_squared_error: 0.8971 - val_loss: 0.5920 - val_mean_squared_error: 0.5920\n",
            "Epoch 31/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8931 - mean_squared_error: 0.8931\n",
            "Epoch 00031: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8941 - mean_squared_error: 0.8941 - val_loss: 0.6059 - val_mean_squared_error: 0.6059\n",
            "Epoch 32/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.8969 - mean_squared_error: 0.8969\n",
            "Epoch 00032: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8959 - mean_squared_error: 0.8959 - val_loss: 0.5845 - val_mean_squared_error: 0.5845\n",
            "Epoch 33/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.8946 - mean_squared_error: 0.8946\n",
            "Epoch 00033: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.8955 - mean_squared_error: 0.8955 - val_loss: 0.6180 - val_mean_squared_error: 0.6180\n",
            "Epoch 34/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.8951 - mean_squared_error: 0.8951\n",
            "Epoch 00034: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8937 - mean_squared_error: 0.8937 - val_loss: 0.5985 - val_mean_squared_error: 0.5985\n",
            "Epoch 35/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8911 - mean_squared_error: 0.8911\n",
            "Epoch 00035: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 253us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 0.6149 - val_mean_squared_error: 0.6149\n",
            "Epoch 36/50\n",
            "17207/17207 [==============================] - ETA: 0s - loss: 0.8933 - mean_squared_error: 0.8933\n",
            "Epoch 00036: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 258us/sample - loss: 0.8933 - mean_squared_error: 0.8933 - val_loss: 0.6009 - val_mean_squared_error: 0.6009\n",
            "Epoch 37/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8926 - mean_squared_error: 0.8926\n",
            "Epoch 00037: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 253us/sample - loss: 0.8928 - mean_squared_error: 0.8928 - val_loss: 0.5894 - val_mean_squared_error: 0.5894\n",
            "Epoch 38/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8933 - mean_squared_error: 0.8933\n",
            "Epoch 00038: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8935 - mean_squared_error: 0.8935 - val_loss: 0.6206 - val_mean_squared_error: 0.6206\n",
            "Epoch 39/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8906 - mean_squared_error: 0.8906\n",
            "Epoch 00039: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 256us/sample - loss: 0.8902 - mean_squared_error: 0.8902 - val_loss: 0.5867 - val_mean_squared_error: 0.5867\n",
            "Epoch 40/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.8943 - mean_squared_error: 0.8943\n",
            "Epoch 00040: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8926 - mean_squared_error: 0.8926 - val_loss: 0.5950 - val_mean_squared_error: 0.5950\n",
            "Epoch 41/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.8903 - mean_squared_error: 0.8903\n",
            "Epoch 00041: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8902 - mean_squared_error: 0.8902 - val_loss: 0.6117 - val_mean_squared_error: 0.6117\n",
            "Epoch 42/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.8886 - mean_squared_error: 0.8886\n",
            "Epoch 00042: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.8901 - mean_squared_error: 0.8901 - val_loss: 0.6159 - val_mean_squared_error: 0.6159\n",
            "Epoch 43/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 0.8873 - mean_squared_error: 0.8873\n",
            "Epoch 00043: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8890 - mean_squared_error: 0.8890 - val_loss: 0.6176 - val_mean_squared_error: 0.6176\n",
            "Epoch 44/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.8894 - mean_squared_error: 0.8894\n",
            "Epoch 00044: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8892 - mean_squared_error: 0.8892 - val_loss: 0.6419 - val_mean_squared_error: 0.6419\n",
            "Epoch 45/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8898 - mean_squared_error: 0.8898\n",
            "Epoch 00045: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 254us/sample - loss: 0.8893 - mean_squared_error: 0.8893 - val_loss: 0.6166 - val_mean_squared_error: 0.6166\n",
            "Epoch 46/50\n",
            "17088/17207 [============================>.] - ETA: 0s - loss: 0.8896 - mean_squared_error: 0.8896\n",
            "Epoch 00046: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 255us/sample - loss: 0.8892 - mean_squared_error: 0.8892 - val_loss: 0.6471 - val_mean_squared_error: 0.6471\n",
            "Epoch 47/50\n",
            "17024/17207 [============================>.] - ETA: 0s - loss: 0.8888 - mean_squared_error: 0.8888\n",
            "Epoch 00047: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 258us/sample - loss: 0.8907 - mean_squared_error: 0.8907 - val_loss: 0.6200 - val_mean_squared_error: 0.6200\n",
            "Epoch 48/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.8887 - mean_squared_error: 0.8887\n",
            "Epoch 00048: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 258us/sample - loss: 0.8883 - mean_squared_error: 0.8883 - val_loss: 0.6011 - val_mean_squared_error: 0.6011\n",
            "Epoch 49/50\n",
            "17184/17207 [============================>.] - ETA: 0s - loss: 0.8883 - mean_squared_error: 0.8883\n",
            "Epoch 00049: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 257us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 0.6390 - val_mean_squared_error: 0.6390\n",
            "Epoch 50/50\n",
            "17152/17207 [============================>.] - ETA: 0s - loss: 0.8894 - mean_squared_error: 0.8894\n",
            "Epoch 00050: val_loss did not improve from 0.56230\n",
            "17207/17207 [==============================] - 4s 258us/sample - loss: 0.8885 - mean_squared_error: 0.8885 - val_loss: 0.5952 - val_mean_squared_error: 0.5952\n"
          ]
        }
      ],
      "source": [
        "print('Train...')\n",
        "history=model.fit(x_train_text, y_train_text,\n",
        "          batch_size=batch_size,\n",
        "          epochs=50,callbacks=[checkpoint],\n",
        "validation_data=(x_val_text, y_val_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlMkJOU4vcNm"
      },
      "outputs": [],
      "source": [
        "mse_train_lstm=history.history['mean_squared_error']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-SBCasZveI4"
      },
      "outputs": [],
      "source": [
        "mse_val_lstm=history.history['val_mean_squared_error']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "dsoz2pPmwFI5",
        "outputId": "6e4bbb95-3169-4afe-cb8e-def74003c10b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mean Square Error of LSTM')"
            ]
          },
          "execution_count": 167,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(mse_train_lstm)),mse_train_lstm,'r')\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('mean square error')\n",
        "plt.plot(range(len(mse_val_lstm)),mse_val_lstm,'b')\n",
        "plt.title('Mean Square Error of LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxfTSPYh4Dni",
        "outputId": "704ab0bb-43cb-4baa-e505-1787f1ea3f50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0,  0, 97],\n",
              "       [ 0, 22, 69]], dtype=int32)"
            ]
          },
          "execution_count": 168,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test_text[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ6RevxEqd9c"
      },
      "source": [
        "test error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMPud2gJpjDs",
        "outputId": "5dc6268f-47fd-42b3-d259-bf0c0a47e75b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8262903425320216"
            ]
          },
          "execution_count": 169,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "y_prd=model.predict(x_test_text)\n",
        "mean_squared_error(y_test_text, y_prd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4yzJgp4JVH",
        "outputId": "9d3510b7-e4cf-4e9a-9465-79dc7a398dee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[3.835919 ],\n",
              "       [4.0168676],\n",
              "       [4.483152 ],\n",
              "       [2.7424881],\n",
              "       [4.305943 ],\n",
              "       [4.5455785],\n",
              "       [3.9788396],\n",
              "       [3.743502 ],\n",
              "       [4.3266573],\n",
              "       [3.743502 ],\n",
              "       [3.743502 ],\n",
              "       [4.4087996],\n",
              "       [4.5416837],\n",
              "       [4.3866453],\n",
              "       [4.196027 ],\n",
              "       [3.9767897],\n",
              "       [4.408564 ],\n",
              "       [4.2162642],\n",
              "       [3.7134106],\n",
              "       [3.9375434]], dtype=float32)"
            ]
          },
          "execution_count": 174,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_prd[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUHDAvHv4MGO",
        "outputId": "a41a50d2-306b-4c47-f844-849521a86c72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 3.5,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 3.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.5,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 4.0,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 3.5,\n",
              " 4.5,\n",
              " ...]"
            ]
          },
          "execution_count": 177,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test_text[20:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO_ohHkR4vWK",
        "outputId": "cd57497f-74a7-427f-fe64-f2d9a1a86b8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Disney',\n",
              " 'good soundtrack',\n",
              " 'Oscar (Best Music - Original Song)',\n",
              " 'predictable',\n",
              " 'clever',\n",
              " 'complicated',\n",
              " 'disturbing',\n",
              " 'fighting',\n",
              " 'great acting',\n",
              " 'helena bonham carter',\n",
              " 'macho',\n",
              " 'mindfuck',\n",
              " 'philosophy',\n",
              " 'powerful ending',\n",
              " 'psychology',\n",
              " 'stylized',\n",
              " 'thought-provoking',\n",
              " 'twist ending',\n",
              " 'violence',\n",
              " 'Al Pacino']"
            ]
          },
          "execution_count": 175,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframe['tag'].tolist()[21508:][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEnaxzCHqiZf"
      },
      "source": [
        "validation errro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo36QYs8F3Aj",
        "outputId": "b7a15c58-1323-49bb-ccd6-65d02c52018b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6429364089071816"
            ]
          },
          "execution_count": 446,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_prd=model.predict(x_val_text)\n",
        "mean_squared_error(y_val_text, y_prd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNLyxUI6qcKm",
        "outputId": "c2c5582d-5ae4-4704-ec35-c7d55591a07a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7203894201638275"
            ]
          },
          "execution_count": 447,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_prd=model.predict(x_train_text)\n",
        "mean_squared_error(y_train_text, y_prd)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}